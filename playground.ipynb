{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "adaptive-financing",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['configs/bowles2021mirabest.cfg']"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import transforms\n",
    "import PIL\n",
    "\n",
    "from torchsummary import summary\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import configparser as ConfigParser\n",
    "\n",
    "\n",
    "import utils\n",
    "# Ipmport various network architectures\n",
    "from networks import AGRadGalNet, VanillaLeNet, testNet, DNSteerableLeNet, DNSteerableAGRadGalNet\n",
    "# Import various data classes\n",
    "from datasets import FRDEEPF\n",
    "from datasets import MiraBest_full, MBFRConfident, MBFRUncertain, MBHybrid\n",
    "from datasets import MingoLoTSS, MLFR, MLFRTest\n",
    "\n",
    "# Set seeds for reproduceability\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# Get correct device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "#device = torch.device(\"cpu\")\n",
    "# Read in config file\n",
    "config_name = [\"bowles2021mirabest.cfg\", \n",
    "               \"bowles2021LoTSS.cfg\", \n",
    "               \"scaife2021mirabest.cfg\", \n",
    "               \"scaife2021mirabestVanilla.cfg\",\n",
    "               \"e2attentionmirabest.cfg\",\n",
    "               \"test.cfg\"\n",
    "              ]\n",
    "config_name = \"configs/\"+config_name[0]\n",
    "config = ConfigParser.ConfigParser(allow_no_value=True)\n",
    "config.read(config_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "filled-territory",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 2 3 4]\n",
      "[ 0  2  4  6  8 10]\n"
     ]
    }
   ],
   "source": [
    "print(np.arange(1,11,2)//2)\n",
    "print(np.arange(1,11,2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "affected-discipline",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading in AGRadGalNet\n",
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1          [-1, 6, 150, 150]              60\n",
      "              ReLU-2          [-1, 6, 150, 150]               0\n",
      "       BatchNorm2d-3          [-1, 6, 150, 150]              12\n",
      "            Conv2d-4          [-1, 6, 150, 150]             330\n",
      "              ReLU-5          [-1, 6, 150, 150]               0\n",
      "       BatchNorm2d-6          [-1, 6, 150, 150]              12\n",
      "            Conv2d-7          [-1, 6, 150, 150]             330\n",
      "              ReLU-8          [-1, 6, 150, 150]               0\n",
      "       BatchNorm2d-9          [-1, 6, 150, 150]              12\n",
      "        MaxPool2d-10            [-1, 6, 75, 75]               0\n",
      "           Conv2d-11           [-1, 16, 75, 75]             880\n",
      "             ReLU-12           [-1, 16, 75, 75]               0\n",
      "      BatchNorm2d-13           [-1, 16, 75, 75]              32\n",
      "           Conv2d-14           [-1, 16, 75, 75]           2,320\n",
      "             ReLU-15           [-1, 16, 75, 75]               0\n",
      "      BatchNorm2d-16           [-1, 16, 75, 75]              32\n",
      "           Conv2d-17           [-1, 16, 75, 75]           2,320\n",
      "             ReLU-18           [-1, 16, 75, 75]               0\n",
      "      BatchNorm2d-19           [-1, 16, 75, 75]              32\n",
      "        MaxPool2d-20           [-1, 16, 37, 37]               0\n",
      "           Conv2d-21           [-1, 32, 37, 37]           4,640\n",
      "             ReLU-22           [-1, 32, 37, 37]               0\n",
      "      BatchNorm2d-23           [-1, 32, 37, 37]              64\n",
      "           Conv2d-24           [-1, 32, 37, 37]           9,248\n",
      "             ReLU-25           [-1, 32, 37, 37]               0\n",
      "      BatchNorm2d-26           [-1, 32, 37, 37]              64\n",
      "           Conv2d-27           [-1, 32, 37, 37]           9,248\n",
      "             ReLU-28           [-1, 32, 37, 37]               0\n",
      "      BatchNorm2d-29           [-1, 32, 37, 37]              64\n",
      "        MaxPool2d-30           [-1, 32, 18, 18]               0\n",
      "           Conv2d-31           [-1, 64, 18, 18]          18,496\n",
      "             ReLU-32           [-1, 64, 18, 18]               0\n",
      "      BatchNorm2d-33           [-1, 64, 18, 18]             128\n",
      "           Conv2d-34           [-1, 64, 18, 18]          36,928\n",
      "             ReLU-35           [-1, 64, 18, 18]               0\n",
      "      BatchNorm2d-36           [-1, 64, 18, 18]             128\n",
      "           Conv2d-37           [-1, 64, 37, 37]           2,048\n",
      "           Conv2d-38           [-1, 64, 18, 18]           4,096\n",
      "         Upsample-39           [-1, 64, 37, 37]               0\n",
      "           Conv2d-40            [-1, 1, 37, 37]              65\n",
      "         Upsample-41            [-1, 1, 37, 37]               0\n",
      "GridAttentionBlock2D-42  [[-1, 32, 37, 37], [-1, 1, 37, 37]]               0\n",
      "           Conv2d-43           [-1, 64, 75, 75]           1,024\n",
      "           Conv2d-44           [-1, 64, 18, 18]           4,096\n",
      "         Upsample-45           [-1, 64, 75, 75]               0\n",
      "           Conv2d-46            [-1, 1, 75, 75]              65\n",
      "         Upsample-47            [-1, 1, 75, 75]               0\n",
      "GridAttentionBlock2D-48  [[-1, 16, 75, 75], [-1, 1, 75, 75]]               0\n",
      "           Conv2d-49         [-1, 64, 150, 150]             384\n",
      "           Conv2d-50           [-1, 64, 18, 18]           4,096\n",
      "         Upsample-51         [-1, 64, 150, 150]               0\n",
      "           Conv2d-52          [-1, 1, 150, 150]              65\n",
      "         Upsample-53          [-1, 1, 150, 150]               0\n",
      "GridAttentionBlock2D-54  [[-1, 6, 150, 150], [-1, 1, 150, 150]]               0\n",
      "          Dropout-55                   [-1, 32]               0\n",
      "          Dropout-56                   [-1, 16]               0\n",
      "          Dropout-57                    [-1, 6]               0\n",
      "           Linear-58                    [-1, 2]              66\n",
      "           Linear-59                    [-1, 2]              34\n",
      "           Linear-60                    [-1, 2]              14\n",
      "           Linear-61                    [-1, 2]              14\n",
      "================================================================\n",
      "Total params: 101,447\n",
      "Trainable params: 101,447\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.09\n",
      "Forward/backward pass size (MB): 27444.59\n",
      "Params size (MB): 0.39\n",
      "Estimated Total Size (MB): 27445.06\n",
      "----------------------------------------------------------------\n",
      "cuda\n",
      "GeForce RTX 2070\n",
      "Files already downloaded and verified\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/raid/scratch/mbowles/EquivariantSelfAttention/venv/lib/python3.8/site-packages/torch/nn/functional.py:3451: UserWarning: Default upsampling behavior when mode=bilinear is changed to align_corners=False since 0.4.0. Please specify align_corners=True if the old behavior is desired. See the documentation of nn.Upsample for details.\n",
      "  warnings.warn(\n",
      "/raid/scratch/mbowles/EquivariantSelfAttention/venv/lib/python3.8/site-packages/torchvision/transforms/transforms.py:1314: UserWarning: Argument resample is deprecated and will be removed since v0.10.0. Please, use interpolation instead\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "quiet = config.getboolean('DEFAULT', 'quiet')\n",
    "early_stopping = config.getboolean('training', 'early_stopping')\n",
    "\n",
    "# Read / Create Folder for Data to be Saved\n",
    "root = config['data']['directory']\n",
    "os.makedirs(root, exist_ok=True)\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Load network architecture (with random seeded weights)\n",
    "print(f\"Loading in {config['model']['base']}\")\n",
    "net = locals()[config['model']['base']](**config['model']).to(device)\n",
    "\n",
    "if not quiet:\n",
    "    if 'DN' not in config['model']['base']:\n",
    "        summary(net, (1, 150, 150))\n",
    "    print(device)\n",
    "    if device == torch.device('cuda'):\n",
    "        print(torch.cuda.get_device_name(device=device))\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Create data transformations\n",
    "datamean = config.getfloat('data', 'datamean')\n",
    "datastd = config.getfloat('data', 'datastd')\n",
    "number_rotations = config.getint('data', 'number_rotations')\n",
    "imsize = config.getint('data', 'imsize')\n",
    "scaling_factor = config.getfloat('data', 'scaling')\n",
    "angles = np.linspace(0, 359, config.getint('data', 'number_rotations'))\n",
    "p_flip = 0.5 if config.getboolean('data','flip') else 0\n",
    "augment = config.getboolean('data', 'augment')\n",
    "\n",
    "# Create hard random (seeded) rotation:\n",
    "class RotationTransform:\n",
    "    \"\"\"Rotate by one of the given angles.\"\"\"\n",
    "    def __init__(self, angles, interpolation):\n",
    "        self.angles = angles\n",
    "        self.interpolation = interpolation\n",
    "\n",
    "    def __call__(self, x):\n",
    "        angle = np.random.choice(a=self.angles, size=1)[0]\n",
    "        return transforms.functional.rotate(x, angle, resample=self.interpolation)\n",
    "\n",
    "# Compose dict of transformations\n",
    "transformations = {\n",
    "    'none': transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([datamean],[datastd])\n",
    "    ]),\n",
    "    'rotation and flipping': transforms.Compose([\n",
    "        transforms.CenterCrop(imsize),\n",
    "        transforms.RandomVerticalFlip(p=p_flip),\n",
    "        RotationTransform(angles, interpolation=PIL.Image.BILINEAR),\n",
    "        transforms.RandomAffine(\n",
    "            degrees=0, # No uncontrolled rotation\n",
    "            scale=(1-scaling_factor, 1+scaling_factor), \n",
    "            resample=PIL.Image.BILINEAR),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([datamean],[datastd])\n",
    "    ]),\n",
    "    'no rotation no flipping': transforms.Compose([\n",
    "        transforms.CenterCrop(imsize),\n",
    "        transforms.RandomVerticalFlip(p=p_flip),\n",
    "        transforms.RandomAffine(\n",
    "            degrees=0, # No uncontrolled rotation\n",
    "            scale=(1-scaling_factor, 1+scaling_factor), \n",
    "            resample=PIL.Image.BILINEAR),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([datamean],[datastd])\n",
    "    ])\n",
    "}\n",
    "\n",
    "download = True\n",
    "train = True\n",
    "data_class = locals()[config['data']['dataset']]\n",
    "\n",
    "if augment and p_flip==0.5:\n",
    "    transform = transformations['rotation and flipping']\n",
    "else:\n",
    "    transform = transformations['no rotation no flipping']\n",
    "\n",
    "train_data = data_class(root=root, download=download, train=True, transform=transform)\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Cross Validation Parameters\n",
    "def fetch_grid_search_param(name, config=config):\n",
    "    raw_ = config['grid_search'][name]\n",
    "    raw_list = raw_.split(',')\n",
    "    do = bool(raw_list.pop(0))\n",
    "    out = np.asarray(raw_list, dtype=np.float64)\n",
    "    return do, out\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Get data parameters\n",
    "batch_size = config.getint('training', 'batch_size')\n",
    "validation_size = config.getfloat('training', 'validation_set_size')\n",
    "dataset_size = len(train_data)\n",
    "\n",
    "nval = int(validation_size*dataset_size)\n",
    "indices = list(range(dataset_size))\n",
    "np.random.shuffle(indices)\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Split Training Data for (Cross) Validation\n",
    "train_indices, val_indices = indices[nval:], indices[:nval]\n",
    "train_sampler = torch.utils.data.Subset(train_data, train_indices)\n",
    "valid_sampler = torch.utils.data.Subset(train_data, val_indices)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_sampler, batch_size=batch_size, shuffle=True)\n",
    "valid_loader = torch.utils.data.DataLoader(valid_sampler, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Extract learning values\n",
    "learning_rate = config.getfloat('training', 'learning_rate')\n",
    "\n",
    "do, lr_scaling = fetch_grid_search_param(name='learning_rate', config=config)\n",
    "learning_rate *= lr_scaling\n",
    "\n",
    "optim_name = config['training']['optimizer']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "juvenile-destination",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Folder to be saved to: models/bowles2021/mirabest\n",
      "Is PATH: True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/raid/scratch/mbowles/EquivariantSelfAttention/venv/lib/python3.8/site-packages/torchvision/transforms/functional.py:935: UserWarning: Argument resample is deprecated and will be removed since v0.10.0. Please, use interpolation instead\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  0\tTraining Loss: 0.823907\t\tValidation Loss: 0.854741\n",
      "\tValidation Loss Down: \t(     inf-->0.854741) ... Updating saved model.\n",
      "Epoch:  1\tTraining Loss: 0.820954\t\tValidation Loss: 0.852446\n",
      "\tValidation Loss Down: \t(0.854741-->0.852446) ... Updating saved model.\n",
      "Epoch:  2\tTraining Loss: 0.820517\t\tValidation Loss: 0.858517\n",
      "Epoch:  3\tTraining Loss: 0.821320\t\tValidation Loss: 0.856533\n",
      "Epoch:  4\tTraining Loss: 0.820001\t\tValidation Loss: 0.858293\n",
      "Epoch:  5\tTraining Loss: 0.816358\t\tValidation Loss: 0.853882\n",
      "Epoch:  6\tTraining Loss: 0.810323\t\tValidation Loss: 0.854228\n",
      "Epoch:  7\tTraining Loss: 0.809393\t\tValidation Loss: 0.848633\n",
      "\tValidation Loss Down: \t(0.852446-->0.848633) ... Updating saved model.\n",
      "Epoch:  8\tTraining Loss: 0.805931\t\tValidation Loss: 0.837892\n",
      "\tValidation Loss Down: \t(0.848633-->0.837892) ... Updating saved model.\n",
      "Epoch:  9\tTraining Loss: 0.808654\t\tValidation Loss: 0.852138\n",
      "Epoch: 10\tTraining Loss: 0.799835\t\tValidation Loss: 0.845425\n",
      "Epoch: 11\tTraining Loss: 0.791415\t\tValidation Loss: 0.839235\n",
      "Epoch: 12\tTraining Loss: 0.803837\t\tValidation Loss: 0.838843\n",
      "Epoch: 13\tTraining Loss: 0.793740\t\tValidation Loss: 0.836470\n",
      "\tValidation Loss Down: \t(0.837892-->0.836470) ... Updating saved model.\n",
      "Epoch: 14\tTraining Loss: 0.788968\t\tValidation Loss: 0.837300\n",
      "Epoch: 15\tTraining Loss: 0.797942\t\tValidation Loss: 0.834253\n",
      "\tValidation Loss Down: \t(0.836470-->0.834253) ... Updating saved model.\n",
      "Epoch: 16\tTraining Loss: 0.790454\t\tValidation Loss: 0.828189\n",
      "\tValidation Loss Down: \t(0.834253-->0.828189) ... Updating saved model.\n",
      "Epoch: 17\tTraining Loss: 0.786925\t\tValidation Loss: 0.834019\n",
      "Epoch: 18\tTraining Loss: 0.792850\t\tValidation Loss: 0.827207\n",
      "\tValidation Loss Down: \t(0.828189-->0.827207) ... Updating saved model.\n",
      "Epoch: 19\tTraining Loss: 0.790056\t\tValidation Loss: 0.829638\n",
      "Epoch: 20\tTraining Loss: 0.785457\t\tValidation Loss: 0.831762\n",
      "Epoch: 21\tTraining Loss: 0.784773\t\tValidation Loss: 0.839531\n",
      "Epoch: 22\tTraining Loss: 0.788123\t\tValidation Loss: 0.831036\n",
      "Epoch: 23\tTraining Loss: 0.778291\t\tValidation Loss: 0.831916\n",
      "Epoch: 24\tTraining Loss: 0.785892\t\tValidation Loss: 0.826244\n",
      "\tValidation Loss Down: \t(0.827207-->0.826244) ... Updating saved model.\n",
      "Epoch: 25\tTraining Loss: 0.782972\t\tValidation Loss: 0.837408\n",
      "Epoch: 26\tTraining Loss: 0.773273\t\tValidation Loss: 0.825986\n",
      "\tValidation Loss Down: \t(0.826244-->0.825986) ... Updating saved model.\n",
      "Epoch: 27\tTraining Loss: 0.767660\t\tValidation Loss: 0.818871\n",
      "\tValidation Loss Down: \t(0.825986-->0.818871) ... Updating saved model.\n",
      "Epoch: 28\tTraining Loss: 0.781779\t\tValidation Loss: 0.826039\n",
      "Epoch: 29\tTraining Loss: 0.775566\t\tValidation Loss: 0.828868\n",
      "Epoch: 30\tTraining Loss: 0.770376\t\tValidation Loss: 0.820731\n",
      "Epoch: 31\tTraining Loss: 0.774873\t\tValidation Loss: 0.814852\n",
      "\tValidation Loss Down: \t(0.818871-->0.814852) ... Updating saved model.\n",
      "Epoch: 32\tTraining Loss: 0.774391\t\tValidation Loss: 0.827688\n",
      "Epoch: 33\tTraining Loss: 0.771152\t\tValidation Loss: 0.819364\n",
      "Epoch: 34\tTraining Loss: 0.766407\t\tValidation Loss: 0.818261\n",
      "Epoch: 35\tTraining Loss: 0.775334\t\tValidation Loss: 0.817663\n",
      "Epoch: 36\tTraining Loss: 0.767291\t\tValidation Loss: 0.808371\n",
      "\tValidation Loss Down: \t(0.814852-->0.808371) ... Updating saved model.\n",
      "Epoch: 37\tTraining Loss: 0.771040\t\tValidation Loss: 0.810942\n",
      "Epoch: 38\tTraining Loss: 0.761511\t\tValidation Loss: 0.790483\n",
      "\tValidation Loss Down: \t(0.808371-->0.790483) ... Updating saved model.\n",
      "Epoch: 39\tTraining Loss: 0.766429\t\tValidation Loss: 0.787310\n",
      "\tValidation Loss Down: \t(0.790483-->0.787310) ... Updating saved model.\n",
      "Epoch: 40\tTraining Loss: 0.765178\t\tValidation Loss: 0.796513\n",
      "Epoch: 41\tTraining Loss: 0.761038\t\tValidation Loss: 0.781899\n",
      "\tValidation Loss Down: \t(0.787310-->0.781899) ... Updating saved model.\n",
      "Epoch: 42\tTraining Loss: 0.752635\t\tValidation Loss: 0.777800\n",
      "\tValidation Loss Down: \t(0.781899-->0.777800) ... Updating saved model.\n",
      "Epoch: 43\tTraining Loss: 0.761271\t\tValidation Loss: 0.792127\n",
      "Epoch: 44\tTraining Loss: 0.756617\t\tValidation Loss: 0.774068\n",
      "\tValidation Loss Down: \t(0.777800-->0.774068) ... Updating saved model.\n",
      "Epoch: 45\tTraining Loss: 0.758563\t\tValidation Loss: 0.778072\n",
      "Epoch: 46\tTraining Loss: 0.761617\t\tValidation Loss: 0.790011\n",
      "Epoch: 47\tTraining Loss: 0.761746\t\tValidation Loss: 0.761073\n",
      "\tValidation Loss Down: \t(0.774068-->0.761073) ... Updating saved model.\n",
      "Epoch: 48\tTraining Loss: 0.752132\t\tValidation Loss: 0.752524\n",
      "\tValidation Loss Down: \t(0.761073-->0.752524) ... Updating saved model.\n",
      "Epoch: 49\tTraining Loss: 0.741910\t\tValidation Loss: 0.758246\n",
      "Epoch: 50\tTraining Loss: 0.751170\t\tValidation Loss: 0.732207\n",
      "\tValidation Loss Down: \t(0.752524-->0.732207) ... Updating saved model.\n",
      "Epoch: 51\tTraining Loss: 0.749066\t\tValidation Loss: 0.730180\n",
      "\tValidation Loss Down: \t(0.732207-->0.730180) ... Updating saved model.\n",
      "Epoch: 52\tTraining Loss: 0.754649\t\tValidation Loss: 0.727631\n",
      "\tValidation Loss Down: \t(0.730180-->0.727631) ... Updating saved model.\n",
      "Epoch: 53\tTraining Loss: 0.748719\t\tValidation Loss: 0.716204\n",
      "\tValidation Loss Down: \t(0.727631-->0.716204) ... Updating saved model.\n",
      "Epoch: 54\tTraining Loss: 0.748187\t\tValidation Loss: 0.725204\n",
      "Epoch: 55\tTraining Loss: 0.747144\t\tValidation Loss: 0.716257\n",
      "Epoch: 56\tTraining Loss: 0.742951\t\tValidation Loss: 0.735438\n",
      "Epoch: 57\tTraining Loss: 0.744612\t\tValidation Loss: 0.755929\n",
      "Epoch: 58\tTraining Loss: 0.743570\t\tValidation Loss: 0.740368\n",
      "Epoch: 59\tTraining Loss: 0.738794\t\tValidation Loss: 0.721194\n",
      "Epoch: 60\tTraining Loss: 0.742071\t\tValidation Loss: 0.727119\n",
      "Epoch: 61\tTraining Loss: 0.736810\t\tValidation Loss: 0.715502\n",
      "\tValidation Loss Down: \t(0.716204-->0.715502) ... Updating saved model.\n",
      "Epoch: 62\tTraining Loss: 0.737700\t\tValidation Loss: 0.716191\n",
      "Epoch: 63\tTraining Loss: 0.732484\t\tValidation Loss: 0.725662\n",
      "Epoch: 64\tTraining Loss: 0.734356\t\tValidation Loss: 0.709838\n",
      "\tValidation Loss Down: \t(0.715502-->0.709838) ... Updating saved model.\n",
      "Epoch: 65\tTraining Loss: 0.738910\t\tValidation Loss: 0.720689\n",
      "Epoch: 66\tTraining Loss: 0.743202\t\tValidation Loss: 0.734287\n",
      "Epoch: 67\tTraining Loss: 0.730204\t\tValidation Loss: 0.734825\n",
      "Epoch: 68\tTraining Loss: 0.732568\t\tValidation Loss: 0.740594\n",
      "Epoch: 69\tTraining Loss: 0.728694\t\tValidation Loss: 0.731395\n",
      "Epoch: 70\tTraining Loss: 0.733797\t\tValidation Loss: 0.731926\n",
      "Epoch: 71\tTraining Loss: 0.734498\t\tValidation Loss: 0.724678\n",
      "Epoch: 72\tTraining Loss: 0.726784\t\tValidation Loss: 0.733256\n",
      "Epoch: 73\tTraining Loss: 0.730616\t\tValidation Loss: 0.710007\n",
      "Epoch: 74\tTraining Loss: 0.726236\t\tValidation Loss: 0.737538\n",
      "Epoch: 75\tTraining Loss: 0.727363\t\tValidation Loss: 0.737695\n",
      "Epoch: 76\tTraining Loss: 0.726128\t\tValidation Loss: 0.730934\n",
      "Epoch: 77\tTraining Loss: 0.717708\t\tValidation Loss: 0.726148\n",
      "Epoch: 78\tTraining Loss: 0.718754\t\tValidation Loss: 0.720536\n",
      "Epoch: 79\tTraining Loss: 0.722009\t\tValidation Loss: 0.718737\n",
      "Epoch: 80\tTraining Loss: 0.714989\t\tValidation Loss: 0.727100\n",
      "Epoch: 81\tTraining Loss: 0.713072\t\tValidation Loss: 0.719913\n",
      "Epoch: 82\tTraining Loss: 0.712611\t\tValidation Loss: 0.723728\n",
      "Epoch: 83\tTraining Loss: 0.707961\t\tValidation Loss: 0.728576\n",
      "Epoch: 84\tTraining Loss: 0.713510\t\tValidation Loss: 0.731054\n",
      "Epoch: 85\tTraining Loss: 0.720170\t\tValidation Loss: 0.729369\n",
      "Epoch: 86\tTraining Loss: 0.717178\t\tValidation Loss: 0.729332\n",
      "Epoch: 87\tTraining Loss: 0.715845\t\tValidation Loss: 0.722360\n",
      "Epoch: 88\tTraining Loss: 0.722443\t\tValidation Loss: 0.717094\n",
      "Epoch: 89\tTraining Loss: 0.713878\t\tValidation Loss: 0.724347\n",
      "Epoch: 90\tTraining Loss: 0.706906\t\tValidation Loss: 0.719312\n",
      "Epoch: 91\tTraining Loss: 0.710844\t\tValidation Loss: 0.719271\n",
      "Epoch: 92\tTraining Loss: 0.709972\t\tValidation Loss: 0.728684\n",
      "Epoch: 93\tTraining Loss: 0.710884\t\tValidation Loss: 0.725398\n",
      "Epoch: 94\tTraining Loss: 0.705630\t\tValidation Loss: 0.732362\n",
      "Epoch: 95\tTraining Loss: 0.713400\t\tValidation Loss: 0.723927\n",
      "Epoch: 96\tTraining Loss: 0.702816\t\tValidation Loss: 0.720137\n",
      "Epoch: 97\tTraining Loss: 0.711203\t\tValidation Loss: 0.718862\n",
      "Epoch: 98\tTraining Loss: 0.704581\t\tValidation Loss: 0.715429\n",
      "Epoch: 99\tTraining Loss: 0.707477\t\tValidation Loss: 0.711410\n",
      "Epoch:100\tTraining Loss: 0.712475\t\tValidation Loss: 0.718461\n",
      "Epoch:101\tTraining Loss: 0.701015\t\tValidation Loss: 0.715808\n",
      "Epoch:102\tTraining Loss: 0.706449\t\tValidation Loss: 0.697493\n",
      "\tValidation Loss Down: \t(0.709838-->0.697493) ... Updating saved model.\n",
      "Epoch:103\tTraining Loss: 0.697732\t\tValidation Loss: 0.708881\n",
      "Epoch:104\tTraining Loss: 0.701519\t\tValidation Loss: 0.712111\n",
      "Epoch:105\tTraining Loss: 0.699769\t\tValidation Loss: 0.712300\n",
      "Epoch:106\tTraining Loss: 0.699757\t\tValidation Loss: 0.717372\n",
      "Epoch:107\tTraining Loss: 0.703194\t\tValidation Loss: 0.708942\n",
      "Epoch:108\tTraining Loss: 0.699480\t\tValidation Loss: 0.702953\n",
      "Epoch:109\tTraining Loss: 0.700668\t\tValidation Loss: 0.699735\n",
      "Epoch:110\tTraining Loss: 0.694528\t\tValidation Loss: 0.711638\n",
      "Epoch:111\tTraining Loss: 0.697103\t\tValidation Loss: 0.709503\n",
      "Epoch:112\tTraining Loss: 0.690376\t\tValidation Loss: 0.703671\n",
      "Epoch:113\tTraining Loss: 0.689754\t\tValidation Loss: 0.697445\n",
      "\tValidation Loss Down: \t(0.697493-->0.697445) ... Updating saved model.\n",
      "Epoch:114\tTraining Loss: 0.690520\t\tValidation Loss: 0.707040\n",
      "Epoch:115\tTraining Loss: 0.682813\t\tValidation Loss: 0.705430\n",
      "Epoch:116\tTraining Loss: 0.698437\t\tValidation Loss: 0.708520\n",
      "Epoch:117\tTraining Loss: 0.693319\t\tValidation Loss: 0.704785\n",
      "Epoch:118\tTraining Loss: 0.693483\t\tValidation Loss: 0.704530\n",
      "Epoch:119\tTraining Loss: 0.688141\t\tValidation Loss: 0.705128\n",
      "Epoch:120\tTraining Loss: 0.687913\t\tValidation Loss: 0.710773\n",
      "Epoch:121\tTraining Loss: 0.692414\t\tValidation Loss: 0.700658\n",
      "Epoch:122\tTraining Loss: 0.689554\t\tValidation Loss: 0.701931\n",
      "Epoch:123\tTraining Loss: 0.687691\t\tValidation Loss: 0.707344\n",
      "Epoch:124\tTraining Loss: 0.684344\t\tValidation Loss: 0.699018\n",
      "Epoch:125\tTraining Loss: 0.686806\t\tValidation Loss: 0.711025\n",
      "Epoch:126\tTraining Loss: 0.688375\t\tValidation Loss: 0.707951\n",
      "Epoch:127\tTraining Loss: 0.689734\t\tValidation Loss: 0.706144\n",
      "Epoch:128\tTraining Loss: 0.683288\t\tValidation Loss: 0.707012\n",
      "Epoch:129\tTraining Loss: 0.686577\t\tValidation Loss: 0.693085\n",
      "\tValidation Loss Down: \t(0.697445-->0.693085) ... Updating saved model.\n",
      "Epoch:130\tTraining Loss: 0.681024\t\tValidation Loss: 0.702267\n",
      "Epoch:131\tTraining Loss: 0.678503\t\tValidation Loss: 0.705609\n",
      "Epoch:132\tTraining Loss: 0.678278\t\tValidation Loss: 0.692658\n",
      "\tValidation Loss Down: \t(0.693085-->0.692658) ... Updating saved model.\n",
      "Epoch:133\tTraining Loss: 0.682114\t\tValidation Loss: 0.711679\n",
      "Epoch:134\tTraining Loss: 0.677654\t\tValidation Loss: 0.696263\n",
      "Epoch:135\tTraining Loss: 0.680871\t\tValidation Loss: 0.704063\n",
      "Epoch:136\tTraining Loss: 0.675689\t\tValidation Loss: 0.710316\n",
      "Epoch:137\tTraining Loss: 0.684994\t\tValidation Loss: 0.696600\n",
      "Epoch:138\tTraining Loss: 0.679820\t\tValidation Loss: 0.690690\n",
      "\tValidation Loss Down: \t(0.692658-->0.690690) ... Updating saved model.\n",
      "Epoch:139\tTraining Loss: 0.674231\t\tValidation Loss: 0.699282\n",
      "Epoch:140\tTraining Loss: 0.674381\t\tValidation Loss: 0.702267\n",
      "Epoch:141\tTraining Loss: 0.677022\t\tValidation Loss: 0.693230\n",
      "Epoch:142\tTraining Loss: 0.673875\t\tValidation Loss: 0.702398\n",
      "Epoch:143\tTraining Loss: 0.675617\t\tValidation Loss: 0.690748\n",
      "Epoch:144\tTraining Loss: 0.676702\t\tValidation Loss: 0.695099\n",
      "Epoch:145\tTraining Loss: 0.675346\t\tValidation Loss: 0.699171\n",
      "Epoch:146\tTraining Loss: 0.676712\t\tValidation Loss: 0.695860\n",
      "Epoch:147\tTraining Loss: 0.675456\t\tValidation Loss: 0.698115\n",
      "Epoch:148\tTraining Loss: 0.673531\t\tValidation Loss: 0.691921\n",
      "Epoch:149\tTraining Loss: 0.674803\t\tValidation Loss: 0.694739\n",
      "Epoch:150\tTraining Loss: 0.673644\t\tValidation Loss: 0.694223\n",
      "Epoch:151\tTraining Loss: 0.674925\t\tValidation Loss: 0.689378\n",
      "\tValidation Loss Down: \t(0.690690-->0.689378) ... Updating saved model.\n",
      "Epoch:152\tTraining Loss: 0.671296\t\tValidation Loss: 0.693930\n",
      "Epoch:153\tTraining Loss: 0.674683\t\tValidation Loss: 0.696765\n",
      "Epoch:154\tTraining Loss: 0.666180\t\tValidation Loss: 0.696552\n",
      "Epoch:155\tTraining Loss: 0.671464\t\tValidation Loss: 0.700907\n",
      "Epoch:156\tTraining Loss: 0.671885\t\tValidation Loss: 0.703147\n",
      "Epoch:157\tTraining Loss: 0.668180\t\tValidation Loss: 0.695048\n",
      "Epoch:158\tTraining Loss: 0.666632\t\tValidation Loss: 0.692005\n",
      "Epoch:159\tTraining Loss: 0.671562\t\tValidation Loss: 0.697133\n",
      "Epoch:160\tTraining Loss: 0.667392\t\tValidation Loss: 0.689848\n",
      "Epoch:161\tTraining Loss: 0.668860\t\tValidation Loss: 0.688847\n",
      "\tValidation Loss Down: \t(0.689378-->0.688847) ... Updating saved model.\n",
      "Epoch:162\tTraining Loss: 0.668958\t\tValidation Loss: 0.691856\n",
      "Epoch:163\tTraining Loss: 0.670288\t\tValidation Loss: 0.694804\n",
      "Epoch:164\tTraining Loss: 0.664790\t\tValidation Loss: 0.692982\n",
      "Epoch:165\tTraining Loss: 0.667131\t\tValidation Loss: 0.692969\n",
      "Epoch:166\tTraining Loss: 0.665912\t\tValidation Loss: 0.696291\n",
      "Epoch:167\tTraining Loss: 0.668531\t\tValidation Loss: 0.691601\n",
      "Epoch:168\tTraining Loss: 0.668643\t\tValidation Loss: 0.700550\n",
      "Epoch:169\tTraining Loss: 0.664791\t\tValidation Loss: 0.695939\n",
      "Epoch:170\tTraining Loss: 0.663479\t\tValidation Loss: 0.698729\n",
      "Epoch:171\tTraining Loss: 0.662270\t\tValidation Loss: 0.690953\n",
      "Epoch:172\tTraining Loss: 0.659800\t\tValidation Loss: 0.690210\n",
      "Epoch:173\tTraining Loss: 0.663214\t\tValidation Loss: 0.697339\n",
      "Epoch:174\tTraining Loss: 0.660614\t\tValidation Loss: 0.685494\n",
      "\tValidation Loss Down: \t(0.688847-->0.685494) ... Updating saved model.\n",
      "Epoch:175\tTraining Loss: 0.663114\t\tValidation Loss: 0.696483\n",
      "Epoch:176\tTraining Loss: 0.663832\t\tValidation Loss: 0.688633\n",
      "Epoch:177\tTraining Loss: 0.660171\t\tValidation Loss: 0.687019\n",
      "Epoch:178\tTraining Loss: 0.663034\t\tValidation Loss: 0.691949\n",
      "Epoch:179\tTraining Loss: 0.661981\t\tValidation Loss: 0.691334\n",
      "Epoch:180\tTraining Loss: 0.653713\t\tValidation Loss: 0.687728\n",
      "Epoch:181\tTraining Loss: 0.656270\t\tValidation Loss: 0.688942\n",
      "Epoch:182\tTraining Loss: 0.656286\t\tValidation Loss: 0.695437\n",
      "Epoch:183\tTraining Loss: 0.657609\t\tValidation Loss: 0.689893\n",
      "Epoch:184\tTraining Loss: 0.655818\t\tValidation Loss: 0.690452\n",
      "Epoch:185\tTraining Loss: 0.656035\t\tValidation Loss: 0.687717\n",
      "Epoch:186\tTraining Loss: 0.657768\t\tValidation Loss: 0.689648\n",
      "Epoch:187\tTraining Loss: 0.659078\t\tValidation Loss: 0.687480\n",
      "Epoch:188\tTraining Loss: 0.658549\t\tValidation Loss: 0.680545\n",
      "\tValidation Loss Down: \t(0.685494-->0.680545) ... Updating saved model.\n",
      "Epoch:189\tTraining Loss: 0.656488\t\tValidation Loss: 0.692298\n",
      "Epoch:190\tTraining Loss: 0.654678\t\tValidation Loss: 0.689480\n",
      "Epoch:191\tTraining Loss: 0.654109\t\tValidation Loss: 0.687162\n",
      "Epoch:192\tTraining Loss: 0.658342\t\tValidation Loss: 0.688016\n",
      "Epoch:193\tTraining Loss: 0.656465\t\tValidation Loss: 0.686896\n",
      "Epoch:194\tTraining Loss: 0.649354\t\tValidation Loss: 0.686090\n",
      "Epoch:195\tTraining Loss: 0.656641\t\tValidation Loss: 0.684877\n",
      "Epoch:196\tTraining Loss: 0.658679\t\tValidation Loss: 0.686388\n",
      "Epoch:197\tTraining Loss: 0.652816\t\tValidation Loss: 0.684414\n",
      "Epoch:198\tTraining Loss: 0.650957\t\tValidation Loss: 0.684320\n",
      "Epoch:199\tTraining Loss: 0.651667\t\tValidation Loss: 0.681495\n",
      "Epoch:200\tTraining Loss: 0.650287\t\tValidation Loss: 0.678081\n",
      "\tValidation Loss Down: \t(0.680545-->0.678081) ... Updating saved model.\n",
      "Epoch:201\tTraining Loss: 0.647769\t\tValidation Loss: 0.684071\n",
      "Epoch:202\tTraining Loss: 0.655304\t\tValidation Loss: 0.687983\n",
      "Epoch:203\tTraining Loss: 0.650243\t\tValidation Loss: 0.678388\n",
      "Epoch:204\tTraining Loss: 0.649885\t\tValidation Loss: 0.686141\n",
      "Epoch:205\tTraining Loss: 0.651269\t\tValidation Loss: 0.680955\n",
      "Epoch:206\tTraining Loss: 0.655786\t\tValidation Loss: 0.683499\n",
      "Epoch:207\tTraining Loss: 0.650149\t\tValidation Loss: 0.685020\n",
      "Epoch:208\tTraining Loss: 0.651266\t\tValidation Loss: 0.686022\n",
      "Epoch:209\tTraining Loss: 0.649962\t\tValidation Loss: 0.678467\n",
      "Epoch:210\tTraining Loss: 0.650415\t\tValidation Loss: 0.689900\n",
      "Epoch:211\tTraining Loss: 0.651310\t\tValidation Loss: 0.690864\n",
      "Epoch:212\tTraining Loss: 0.653063\t\tValidation Loss: 0.683905\n",
      "Epoch:213\tTraining Loss: 0.647302\t\tValidation Loss: 0.680559\n",
      "Epoch:214\tTraining Loss: 0.652808\t\tValidation Loss: 0.682456\n",
      "Epoch:215\tTraining Loss: 0.647280\t\tValidation Loss: 0.677251\n",
      "\tValidation Loss Down: \t(0.678081-->0.677251) ... Updating saved model.\n",
      "Epoch:216\tTraining Loss: 0.648637\t\tValidation Loss: 0.681278\n",
      "Epoch:217\tTraining Loss: 0.651576\t\tValidation Loss: 0.683235\n",
      "Epoch:218\tTraining Loss: 0.648100\t\tValidation Loss: 0.683889\n",
      "Epoch:219\tTraining Loss: 0.649793\t\tValidation Loss: 0.683474\n",
      "Epoch:220\tTraining Loss: 0.651772\t\tValidation Loss: 0.683119\n",
      "Epoch:221\tTraining Loss: 0.650605\t\tValidation Loss: 0.686774\n",
      "Epoch:222\tTraining Loss: 0.647789\t\tValidation Loss: 0.685753\n",
      "Epoch:223\tTraining Loss: 0.648592\t\tValidation Loss: 0.680148\n",
      "Epoch:224\tTraining Loss: 0.646844\t\tValidation Loss: 0.678565\n",
      "Epoch:225\tTraining Loss: 0.646416\t\tValidation Loss: 0.681990\n",
      "Epoch:226\tTraining Loss: 0.652921\t\tValidation Loss: 0.692413\n",
      "Epoch:227\tTraining Loss: 0.646158\t\tValidation Loss: 0.688856\n",
      "Epoch:228\tTraining Loss: 0.647054\t\tValidation Loss: 0.677536\n",
      "Epoch:229\tTraining Loss: 0.647334\t\tValidation Loss: 0.680771\n",
      "Epoch:230\tTraining Loss: 0.649552\t\tValidation Loss: 0.687079\n",
      "Epoch:231\tTraining Loss: 0.647436\t\tValidation Loss: 0.682616\n",
      "Epoch:232\tTraining Loss: 0.645285\t\tValidation Loss: 0.679588\n",
      "Epoch:233\tTraining Loss: 0.645852\t\tValidation Loss: 0.681539\n",
      "Epoch:234\tTraining Loss: 0.645627\t\tValidation Loss: 0.685134\n",
      "Epoch:235\tTraining Loss: 0.647674\t\tValidation Loss: 0.679523\n",
      "Epoch:236\tTraining Loss: 0.645967\t\tValidation Loss: 0.683444\n",
      "Epoch:237\tTraining Loss: 0.643142\t\tValidation Loss: 0.683532\n",
      "Epoch:238\tTraining Loss: 0.643772\t\tValidation Loss: 0.687526\n",
      "Epoch:239\tTraining Loss: 0.644425\t\tValidation Loss: 0.681788\n",
      "Epoch:240\tTraining Loss: 0.643266\t\tValidation Loss: 0.684821\n",
      "Epoch:241\tTraining Loss: 0.640536\t\tValidation Loss: 0.682923\n",
      "Epoch:242\tTraining Loss: 0.647686\t\tValidation Loss: 0.691322\n",
      "Epoch:243\tTraining Loss: 0.644241\t\tValidation Loss: 0.691901\n",
      "Epoch:244\tTraining Loss: 0.643012\t\tValidation Loss: 0.687459\n",
      "Epoch:245\tTraining Loss: 0.645072\t\tValidation Loss: 0.683090\n",
      "Epoch:246\tTraining Loss: 0.643657\t\tValidation Loss: 0.687800\n",
      "Epoch:247\tTraining Loss: 0.643442\t\tValidation Loss: 0.686682\n",
      "Epoch:248\tTraining Loss: 0.642241\t\tValidation Loss: 0.687073\n",
      "Epoch:249\tTraining Loss: 0.644208\t\tValidation Loss: 0.685876\n",
      "Epoch:250\tTraining Loss: 0.642205\t\tValidation Loss: 0.685286\n",
      "Epoch:251\tTraining Loss: 0.641057\t\tValidation Loss: 0.688684\n",
      "Epoch:252\tTraining Loss: 0.644310\t\tValidation Loss: 0.679662\n",
      "Epoch:253\tTraining Loss: 0.641927\t\tValidation Loss: 0.684625\n",
      "Epoch:254\tTraining Loss: 0.637574\t\tValidation Loss: 0.682662\n",
      "Epoch:255\tTraining Loss: 0.640986\t\tValidation Loss: 0.682198\n",
      "Epoch:256\tTraining Loss: 0.645474\t\tValidation Loss: 0.680342\n",
      "Epoch:257\tTraining Loss: 0.641135\t\tValidation Loss: 0.684683\n",
      "Epoch:258\tTraining Loss: 0.639628\t\tValidation Loss: 0.683098\n",
      "Epoch:259\tTraining Loss: 0.639949\t\tValidation Loss: 0.679936\n",
      "Epoch:260\tTraining Loss: 0.638771\t\tValidation Loss: 0.683210\n",
      "Epoch:261\tTraining Loss: 0.636090\t\tValidation Loss: 0.684908\n",
      "Epoch:262\tTraining Loss: 0.638109\t\tValidation Loss: 0.689642\n",
      "Epoch:263\tTraining Loss: 0.637906\t\tValidation Loss: 0.681348\n",
      "Epoch:264\tTraining Loss: 0.635822\t\tValidation Loss: 0.686164\n",
      "Epoch:265\tTraining Loss: 0.640727\t\tValidation Loss: 0.687348\n",
      "Epoch:266\tTraining Loss: 0.639084\t\tValidation Loss: 0.689043\n",
      "Epoch:267\tTraining Loss: 0.635868\t\tValidation Loss: 0.684524\n",
      "Epoch:268\tTraining Loss: 0.639007\t\tValidation Loss: 0.685351\n",
      "Epoch:269\tTraining Loss: 0.637094\t\tValidation Loss: 0.687838\n",
      "Epoch:270\tTraining Loss: 0.638901\t\tValidation Loss: 0.688347\n",
      "Epoch:271\tTraining Loss: 0.638170\t\tValidation Loss: 0.690786\n",
      "Epoch:272\tTraining Loss: 0.636717\t\tValidation Loss: 0.684728\n",
      "Epoch:273\tTraining Loss: 0.637326\t\tValidation Loss: 0.690977\n",
      "Epoch:274\tTraining Loss: 0.632879\t\tValidation Loss: 0.690408\n",
      "Epoch:275\tTraining Loss: 0.641027\t\tValidation Loss: 0.685008\n",
      "Epoch:276\tTraining Loss: 0.634450\t\tValidation Loss: 0.686603\n",
      "Epoch:277\tTraining Loss: 0.634328\t\tValidation Loss: 0.682196\n",
      "Epoch:278\tTraining Loss: 0.635374\t\tValidation Loss: 0.681226\n",
      "Epoch:279\tTraining Loss: 0.636802\t\tValidation Loss: 0.683746\n",
      "Epoch:280\tTraining Loss: 0.635473\t\tValidation Loss: 0.684228\n",
      "Epoch:281\tTraining Loss: 0.632677\t\tValidation Loss: 0.683364\n",
      "Epoch:282\tTraining Loss: 0.636785\t\tValidation Loss: 0.678477\n",
      "Epoch:283\tTraining Loss: 0.634669\t\tValidation Loss: 0.683700\n",
      "Epoch:284\tTraining Loss: 0.633056\t\tValidation Loss: 0.685869\n",
      "Epoch:285\tTraining Loss: 0.634939\t\tValidation Loss: 0.680051\n",
      "Epoch:286\tTraining Loss: 0.633725\t\tValidation Loss: 0.683946\n",
      "Epoch:287\tTraining Loss: 0.633391\t\tValidation Loss: 0.685524\n",
      "Epoch:288\tTraining Loss: 0.636810\t\tValidation Loss: 0.685485\n",
      "Epoch:289\tTraining Loss: 0.634322\t\tValidation Loss: 0.687188\n",
      "Epoch:290\tTraining Loss: 0.634221\t\tValidation Loss: 0.682893\n",
      "Epoch:291\tTraining Loss: 0.632790\t\tValidation Loss: 0.675895\n",
      "\tValidation Loss Down: \t(0.677251-->0.675895) ... Updating saved model.\n",
      "Epoch:292\tTraining Loss: 0.633837\t\tValidation Loss: 0.686681\n",
      "Epoch:293\tTraining Loss: 0.636041\t\tValidation Loss: 0.680095\n",
      "Epoch:294\tTraining Loss: 0.629990\t\tValidation Loss: 0.680698\n",
      "Epoch:295\tTraining Loss: 0.631156\t\tValidation Loss: 0.679427\n",
      "Epoch:296\tTraining Loss: 0.631702\t\tValidation Loss: 0.678181\n",
      "Epoch:297\tTraining Loss: 0.629481\t\tValidation Loss: 0.679668\n",
      "Epoch:298\tTraining Loss: 0.629745\t\tValidation Loss: 0.674353\n",
      "\tValidation Loss Down: \t(0.675895-->0.674353) ... Updating saved model.\n",
      "Epoch:299\tTraining Loss: 0.632072\t\tValidation Loss: 0.676893\n",
      "Epoch:300\tTraining Loss: 0.628531\t\tValidation Loss: 0.672362\n",
      "\tValidation Loss Down: \t(0.674353-->0.672362) ... Updating saved model.\n",
      "Epoch:301\tTraining Loss: 0.630110\t\tValidation Loss: 0.679696\n",
      "Epoch:302\tTraining Loss: 0.628518\t\tValidation Loss: 0.679424\n",
      "Epoch:303\tTraining Loss: 0.628180\t\tValidation Loss: 0.678917\n",
      "Epoch:304\tTraining Loss: 0.629172\t\tValidation Loss: 0.669984\n",
      "\tValidation Loss Down: \t(0.672362-->0.669984) ... Updating saved model.\n",
      "Epoch:305\tTraining Loss: 0.631580\t\tValidation Loss: 0.678361\n",
      "Epoch:306\tTraining Loss: 0.627134\t\tValidation Loss: 0.676882\n",
      "Epoch:307\tTraining Loss: 0.628708\t\tValidation Loss: 0.687742\n",
      "Epoch:308\tTraining Loss: 0.630269\t\tValidation Loss: 0.667360\n",
      "\tValidation Loss Down: \t(0.669984-->0.667360) ... Updating saved model.\n",
      "Epoch:309\tTraining Loss: 0.632106\t\tValidation Loss: 0.679399\n",
      "Epoch:310\tTraining Loss: 0.628517\t\tValidation Loss: 0.679630\n",
      "Epoch:311\tTraining Loss: 0.627609\t\tValidation Loss: 0.686199\n",
      "Epoch:312\tTraining Loss: 0.629146\t\tValidation Loss: 0.678535\n",
      "Epoch:313\tTraining Loss: 0.629247\t\tValidation Loss: 0.677830\n",
      "Epoch:314\tTraining Loss: 0.627585\t\tValidation Loss: 0.682043\n",
      "Epoch:315\tTraining Loss: 0.628962\t\tValidation Loss: 0.681702\n",
      "Epoch:316\tTraining Loss: 0.627277\t\tValidation Loss: 0.679272\n",
      "Epoch:317\tTraining Loss: 0.627519\t\tValidation Loss: 0.674454\n",
      "Epoch:318\tTraining Loss: 0.628766\t\tValidation Loss: 0.687349\n",
      "Epoch:319\tTraining Loss: 0.629586\t\tValidation Loss: 0.683810\n",
      "Epoch:320\tTraining Loss: 0.631728\t\tValidation Loss: 0.674900\n",
      "Epoch:321\tTraining Loss: 0.629470\t\tValidation Loss: 0.669138\n",
      "Epoch:322\tTraining Loss: 0.623125\t\tValidation Loss: 0.680323\n",
      "Epoch:323\tTraining Loss: 0.625387\t\tValidation Loss: 0.674694\n",
      "Epoch:324\tTraining Loss: 0.626585\t\tValidation Loss: 0.678698\n",
      "Epoch:325\tTraining Loss: 0.629018\t\tValidation Loss: 0.680797\n",
      "Epoch:326\tTraining Loss: 0.628395\t\tValidation Loss: 0.682343\n",
      "Epoch:327\tTraining Loss: 0.626650\t\tValidation Loss: 0.673843\n",
      "Epoch:328\tTraining Loss: 0.622838\t\tValidation Loss: 0.679746\n",
      "Epoch:329\tTraining Loss: 0.627702\t\tValidation Loss: 0.674375\n",
      "Epoch:330\tTraining Loss: 0.622420\t\tValidation Loss: 0.672887\n",
      "Epoch:331\tTraining Loss: 0.626166\t\tValidation Loss: 0.673456\n",
      "Epoch:332\tTraining Loss: 0.628500\t\tValidation Loss: 0.668922\n",
      "Epoch:333\tTraining Loss: 0.621418\t\tValidation Loss: 0.675998\n",
      "Epoch:334\tTraining Loss: 0.625614\t\tValidation Loss: 0.673805\n",
      "Epoch:335\tTraining Loss: 0.625126\t\tValidation Loss: 0.677911\n",
      "Epoch:336\tTraining Loss: 0.621367\t\tValidation Loss: 0.680151\n",
      "Epoch:337\tTraining Loss: 0.622128\t\tValidation Loss: 0.677315\n",
      "Epoch:338\tTraining Loss: 0.623162\t\tValidation Loss: 0.677542\n",
      "Epoch:339\tTraining Loss: 0.622633\t\tValidation Loss: 0.668046\n",
      "Epoch:340\tTraining Loss: 0.626971\t\tValidation Loss: 0.666240\n",
      "\tValidation Loss Down: \t(0.667360-->0.666240) ... Updating saved model.\n",
      "Epoch:341\tTraining Loss: 0.625585\t\tValidation Loss: 0.682034\n",
      "Epoch:342\tTraining Loss: 0.626716\t\tValidation Loss: 0.681709\n",
      "Epoch:343\tTraining Loss: 0.620282\t\tValidation Loss: 0.670234\n",
      "Epoch:344\tTraining Loss: 0.623772\t\tValidation Loss: 0.669627\n",
      "Epoch:345\tTraining Loss: 0.624397\t\tValidation Loss: 0.669450\n",
      "Epoch:346\tTraining Loss: 0.620718\t\tValidation Loss: 0.657053\n",
      "\tValidation Loss Down: \t(0.666240-->0.657053) ... Updating saved model.\n",
      "Epoch:347\tTraining Loss: 0.621315\t\tValidation Loss: 0.677002\n",
      "Epoch:348\tTraining Loss: 0.622203\t\tValidation Loss: 0.672662\n",
      "Epoch:349\tTraining Loss: 0.621056\t\tValidation Loss: 0.674851\n",
      "Epoch:350\tTraining Loss: 0.619270\t\tValidation Loss: 0.671794\n",
      "Epoch:351\tTraining Loss: 0.618012\t\tValidation Loss: 0.679317\n",
      "Epoch:352\tTraining Loss: 0.622406\t\tValidation Loss: 0.676078\n",
      "Epoch:353\tTraining Loss: 0.623966\t\tValidation Loss: 0.671748\n",
      "Epoch:354\tTraining Loss: 0.620576\t\tValidation Loss: 0.674882\n",
      "Epoch:355\tTraining Loss: 0.622173\t\tValidation Loss: 0.678167\n",
      "Epoch:356\tTraining Loss: 0.620427\t\tValidation Loss: 0.677902\n",
      "Epoch:357\tTraining Loss: 0.617488\t\tValidation Loss: 0.675166\n",
      "Epoch:358\tTraining Loss: 0.621746\t\tValidation Loss: 0.684510\n",
      "Epoch:359\tTraining Loss: 0.619805\t\tValidation Loss: 0.671450\n",
      "Epoch:360\tTraining Loss: 0.621167\t\tValidation Loss: 0.670291\n",
      "Epoch:361\tTraining Loss: 0.617084\t\tValidation Loss: 0.663158\n",
      "Epoch:362\tTraining Loss: 0.617211\t\tValidation Loss: 0.677635\n",
      "Epoch:363\tTraining Loss: 0.614219\t\tValidation Loss: 0.665156\n",
      "Epoch:364\tTraining Loss: 0.620023\t\tValidation Loss: 0.679142\n",
      "Epoch:365\tTraining Loss: 0.621446\t\tValidation Loss: 0.673987\n",
      "Epoch:366\tTraining Loss: 0.619758\t\tValidation Loss: 0.675619\n",
      "Epoch:367\tTraining Loss: 0.618649\t\tValidation Loss: 0.664644\n",
      "Epoch:368\tTraining Loss: 0.614284\t\tValidation Loss: 0.675771\n",
      "Epoch:369\tTraining Loss: 0.616841\t\tValidation Loss: 0.677924\n",
      "Epoch:370\tTraining Loss: 0.619702\t\tValidation Loss: 0.667108\n",
      "Epoch:371\tTraining Loss: 0.613712\t\tValidation Loss: 0.663979\n",
      "Epoch:372\tTraining Loss: 0.616570\t\tValidation Loss: 0.679382\n",
      "Epoch:373\tTraining Loss: 0.615911\t\tValidation Loss: 0.670803\n",
      "Epoch:374\tTraining Loss: 0.612120\t\tValidation Loss: 0.678689\n",
      "Epoch:375\tTraining Loss: 0.614233\t\tValidation Loss: 0.673107\n",
      "Epoch:376\tTraining Loss: 0.614328\t\tValidation Loss: 0.676500\n",
      "Epoch:377\tTraining Loss: 0.613672\t\tValidation Loss: 0.672268\n",
      "Epoch:378\tTraining Loss: 0.617331\t\tValidation Loss: 0.676468\n",
      "Epoch:379\tTraining Loss: 0.616494\t\tValidation Loss: 0.665931\n",
      "Epoch:380\tTraining Loss: 0.618595\t\tValidation Loss: 0.671831\n",
      "Epoch:381\tTraining Loss: 0.615890\t\tValidation Loss: 0.669843\n",
      "Epoch:382\tTraining Loss: 0.616961\t\tValidation Loss: 0.671876\n",
      "Epoch:383\tTraining Loss: 0.613524\t\tValidation Loss: 0.673273\n",
      "Epoch:384\tTraining Loss: 0.613831\t\tValidation Loss: 0.676237\n",
      "Epoch:385\tTraining Loss: 0.610995\t\tValidation Loss: 0.667056\n",
      "Epoch:386\tTraining Loss: 0.615526\t\tValidation Loss: 0.672462\n",
      "Epoch:387\tTraining Loss: 0.612500\t\tValidation Loss: 0.664381\n",
      "Epoch:388\tTraining Loss: 0.615188\t\tValidation Loss: 0.668498\n",
      "Epoch:389\tTraining Loss: 0.614899\t\tValidation Loss: 0.680499\n",
      "Epoch:390\tTraining Loss: 0.612707\t\tValidation Loss: 0.681802\n",
      "Epoch:391\tTraining Loss: 0.614260\t\tValidation Loss: 0.672653\n",
      "Epoch:392\tTraining Loss: 0.616585\t\tValidation Loss: 0.677962\n",
      "Epoch:393\tTraining Loss: 0.611858\t\tValidation Loss: 0.679653\n",
      "Epoch:394\tTraining Loss: 0.616867\t\tValidation Loss: 0.672759\n",
      "Epoch:395\tTraining Loss: 0.613501\t\tValidation Loss: 0.671776\n",
      "Epoch:396\tTraining Loss: 0.616648\t\tValidation Loss: 0.663675\n",
      "Epoch:397\tTraining Loss: 0.611994\t\tValidation Loss: 0.670204\n",
      "Epoch:398\tTraining Loss: 0.610740\t\tValidation Loss: 0.678648\n",
      "Epoch:399\tTraining Loss: 0.608062\t\tValidation Loss: 0.670152\n",
      "Epoch:400\tTraining Loss: 0.612198\t\tValidation Loss: 0.669368\n",
      "Epoch:401\tTraining Loss: 0.613492\t\tValidation Loss: 0.670983\n",
      "Epoch:402\tTraining Loss: 0.615587\t\tValidation Loss: 0.673840\n",
      "Epoch:403\tTraining Loss: 0.609412\t\tValidation Loss: 0.672881\n",
      "Epoch:404\tTraining Loss: 0.611519\t\tValidation Loss: 0.671369\n",
      "Epoch:405\tTraining Loss: 0.608569\t\tValidation Loss: 0.658120\n",
      "Epoch:406\tTraining Loss: 0.608210\t\tValidation Loss: 0.668938\n",
      "Epoch:407\tTraining Loss: 0.608633\t\tValidation Loss: 0.664730\n",
      "Epoch:408\tTraining Loss: 0.609542\t\tValidation Loss: 0.675271\n",
      "Epoch:409\tTraining Loss: 0.605383\t\tValidation Loss: 0.668178\n",
      "Epoch:410\tTraining Loss: 0.615066\t\tValidation Loss: 0.672390\n",
      "Epoch:411\tTraining Loss: 0.611213\t\tValidation Loss: 0.670056\n",
      "Epoch:412\tTraining Loss: 0.607964\t\tValidation Loss: 0.672459\n",
      "Epoch:413\tTraining Loss: 0.608339\t\tValidation Loss: 0.666122\n",
      "Epoch:414\tTraining Loss: 0.606863\t\tValidation Loss: 0.670587\n",
      "Epoch:415\tTraining Loss: 0.607134\t\tValidation Loss: 0.676719\n"
     ]
    }
   ],
   "source": [
    "# -----------------------------------------------------------------------------\n",
    "# Training function test\n",
    "weight_decay = config.getfloat('training', 'weight_decay')\n",
    "lr = config.getfloat('final_parameters', 'learning_rate')\n",
    "optimizers = {\n",
    "    'SGD': optim.SGD(net.parameters(), lr=lr, momentum=0.9),\n",
    "    'Adagrad': optim.Adagrad(net.parameters(), lr=lr),\n",
    "    'Adadelta': optim.Adadelta(net.parameters(), lr=lr),\n",
    "    'Adam': optim.Adam(net.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "    }\n",
    "optimizer  = optimizers[optim_name]\n",
    "model, conf_mat, validatopn_min = utils.train(\n",
    "    net,\n",
    "    device,\n",
    "    config,\n",
    "    train_loader,\n",
    "    valid_loader,\n",
    "    optimizer=optimizer,\n",
    "    root_out_directory_addition=f'',\n",
    "    scheduler = None,\n",
    "    save_validation_updates=True,\n",
    "    class_splitting_index=1,\n",
    "    loss_function=nn.CrossEntropyLoss(),\n",
    "    output_model=True,\n",
    "    early_stopping=True,\n",
    "    output_best_validation=True        \n",
    ")\n",
    "print(f\"\"\"Confusion Matrix: {conf_mat}\n",
    "Learning Rate: {lr}\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "raw",
   "id": "antique-designation",
   "metadata": {
    "tags": []
   },
   "source": [
    "# -----------------------------------------------------------------------------\n",
    "# Training function test (with lr search)\n",
    "weight_decay = config.getfloat('training', 'weight_decay')\n",
    "if not config.getboolean('grid_search', 'done'):\n",
    "    for lr in learning_rate:\n",
    "        optimizers = {\n",
    "            'SGD': optim.SGD(net.parameters(), lr=lr, momentum=0.9),\n",
    "            'Adagrad': optim.Adagrad(net.parameters(), lr=lr),\n",
    "            'Adadelta': optim.Adadelta(net.parameters(), lr=lr),\n",
    "            'Adam': optim.Adam(net.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "            }\n",
    "        optimizer  = optimizers[optim_name]\n",
    "        model, conf_mat, validatopn_min = utils.train(\n",
    "            net,\n",
    "            device,\n",
    "            config,\n",
    "            train_loader,\n",
    "            valid_loader,\n",
    "            optimizer=optimizer,\n",
    "            root_out_directory_addition=f'/search_lr_{lr}',\n",
    "            scheduler = None,\n",
    "            save_validation_updates=True,\n",
    "            class_splitting_index=1,\n",
    "            loss_function=nn.CrossEntropyLoss(),\n",
    "            output_model=True,\n",
    "            early_stopping=True,\n",
    "            output_best_validation=True        \n",
    "        )\n",
    "        print(f\"\"\"Confusion Matrix: {conf_mat}\n",
    "        Learning Rate: {lr}\n",
    "        \"\"\")\n",
    "else:\n",
    "    lr = config.getfloat('final_parameters', 'learning_rate')\n",
    "    optimizers = {\n",
    "        'SGD': optim.SGD(net.parameters(), lr=lr, momentum=0.9),\n",
    "        'Adagrad': optim.Adagrad(net.parameters(), lr=lr),\n",
    "        'Adadelta': optim.Adadelta(net.parameters(), lr=lr),\n",
    "        'Adam': optim.Adam(net.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "        }\n",
    "    optimizer  = optimizers[optim_name]\n",
    "    model, conf_mat, validatopn_min = utils.train(\n",
    "        net,\n",
    "        device,\n",
    "        config,\n",
    "        train_loader,\n",
    "        valid_loader,\n",
    "        optimizer=optimizer,\n",
    "        root_out_directory_addition=f'',\n",
    "        scheduler = None,\n",
    "        save_validation_updates=True,\n",
    "        class_splitting_index=1,\n",
    "        loss_function=nn.CrossEntropyLoss(),\n",
    "        output_model=True,\n",
    "        early_stopping=True,\n",
    "        output_best_validation=True        \n",
    "    )\n",
    "    print(f\"\"\"Confusion Matrix: {conf_mat}\n",
    "    Learning Rate: {lr}\n",
    "    \"\"\")"
   ]
  },
  {
   "cell_type": "raw",
   "id": "coated-electronics",
   "metadata": {},
   "source": [
    "# THIS IS UNFINISHED\n",
    "grid_search_done = config.getboolean('grid_search', 'done')\n",
    "\n",
    "def parameter_grid_creation(strategy='None'):\n",
    "    params_ = {'learning_rate': 0}\n",
    "    grid = []\n",
    "    hyperparams = {}\n",
    "    grid_search_parameters = config['grid_search']['hyperparameters'].strip('[]').replace(' ', '').split(',')\n",
    "    \n",
    "    # Create dict of values for grid searching\n",
    "    for hparam in grid_search_parameters:\n",
    "        do, values = fetch_grid_search_param(name=hparam, config=config)\n",
    "        if do:\n",
    "            hyperparams[hparam]=list(values)\n",
    "    print(hyperparams)\n",
    "    \n",
    "\n",
    "    \n",
    "    # Naive / full grid search approach\n",
    "    for key in grid_search_parameters:\n",
    "        print(key)\n",
    "    if strategy == 'None':\n",
    "        grid = []\n",
    "        for key in grid_search_parameters:\n",
    "            tmp_ = params_\n",
    "            for element in hyperparams[key]:\n",
    "                tmp_[key] = element\n",
    "                grid.append(tmp_.copy())\n",
    "    \n",
    "    # Calculate size of space to search over\n",
    "    #search_size = 1\n",
    "    #for key in hyperparams.keys():\n",
    "    #    search_size *= len(hyperparams[key])\n",
    "    # n_folds is the number of data subsets we are initialising for grid search.\n",
    "    # We will use the size of \n",
    "    # 8 maximum folds: dont want validation to be much smaller than test set (calculated on MiraBest) and even number.\n",
    "    #n_folds = 8 if search_size>=8 else search_size\n",
    "    return grid\n",
    "print(parameter_grid_creation())"
   ]
  },
  {
   "cell_type": "raw",
   "id": "confirmed-shelter",
   "metadata": {},
   "source": [
    "def fetch_grid_search_param(name, config=config):\n",
    "    raw_ = config['grid_search'][name]\n",
    "    raw_list = raw_.split(',')\n",
    "    do = bool(raw_list.pop(0))\n",
    "    out = np.asarray(raw_list, dtype=np.float64)\n",
    "    return do, out\n",
    "\n",
    "grid_search_parameters = config['grid_search']['hyperparameters'].strip('[]').replace(' ', '').split(',')\n",
    "print(grid_search_parameters)\n",
    "hyperparams = {}\n",
    "for hparam in grid_search_parameters:\n",
    "    do, values = fetch_grid_search_param(name=hparam, config=config)\n",
    "    if do:\n",
    "        hyperparams[hparam]=list(values)\n",
    "print(hyperparams)\n",
    "\n",
    "search_size = 1\n",
    "for key in hyperparams.keys():\n",
    "    search_size *= len(hyperparams[key])\n",
    "# 8 selected, as even number and MiraBest val is approx equal MiraBest test at 7.\n",
    "n_folds = 8 if search_size>=8 else search_size \n",
    "print(n_folds)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "increased-ethernet",
   "metadata": {},
   "source": [
    "grid_search_done = config.getboolean('grid_search', 'done')\n",
    "\n",
    "if grid_search_done:\n",
    "    pass\n",
    "else:\n",
    "    grid_search_parameters = config['grid_search']['hyperparameters'].strip('[]').replace(' ', '').split(',')\n",
    "    hyperparams = {}\n",
    "    for hparam in grid_search_parameters:\n",
    "        do, values = fetch_grid_search_param(name=hparam, config=config)\n",
    "        if do:\n",
    "            hyperparams[hparam]=list(values)\n",
    "    print(hyperparams)\n",
    "\n",
    "    # Calculate size of space to search over\n",
    "    search_size = 1\n",
    "    for key in hyperparams.keys():\n",
    "        search_size *= len(hyperparams[key])\n",
    "    \n",
    "    # n_folds is the number of data subsets we are initialising for grid search.\n",
    "    # We will use the size of \n",
    "    # 8 maximum folds: dont want validation to be much smaller than test set (calculated on MiraBest) and even number.\n",
    "    n_folds = 8 if search_size>=8 else search_size"
   ]
  },
  {
   "cell_type": "raw",
   "id": "authentic-fruit",
   "metadata": {},
   "source": [
    "# -----------------------------------------------------------------------------\n",
    "# Get Training Hyperparameters\n",
    "batch_size = config.getint('training', 'batch_size')\n",
    "validation_size = config.getfloat('training', 'validation_set_size')\n",
    "\n",
    "# k_fold cross validation\n",
    "def kfold_cross_split(data, k=8, n=[]):\n",
    "    if n==[]:\n",
    "        n = range(k)\n",
    "    data_size = len(data)\n",
    "    n_samples = data_size//k\n",
    "    \n",
    "    indices = list(range(data_size))\n",
    "    np.random.shuffle(indices)\n",
    "    train_loader = []\n",
    "    valid_loader = []\n",
    "    \n",
    "    if type(n)==int:\n",
    "        n = [n]\n",
    "    for i in n:\n",
    "        if i<k:\n",
    "            # Generate the i-th split and append it as data loaders\n",
    "            train_indices = indices[:i*n_samples]+indices[(i+1)*n_samples:]\n",
    "            val_indices   = indices[i*n_samples:(i+1)*n_samples]\n",
    "            train_sampler = torch.utils.data.Subset(train_data, train_indices)\n",
    "            valid_sampler = torch.utils.data.Subset(train_data, val_indices)\n",
    "            train_loader.append(torch.utils.data.DataLoader(train_sampler, batch_size=batch_size, shuffle=True))\n",
    "            valid_loader.append(torch.utils.data.DataLoader(valid_sampler, batch_size=batch_size, shuffle=True))\n",
    "    \n",
    "    train_loader = train_loader[0] if len(train_loader)==1 else train_loader\n",
    "    valid_loader = valid_loader[0] if len(valid_loader)==1 else valid_loader\n",
    "    \n",
    "    return train_loader, valid_loader\n",
    "    \n",
    "train_loader, valid_loader = kfold_cross_split(data=train_data, k=n_folds, n=[])\n",
    "print(type(train_loader))"
   ]
  },
  {
   "cell_type": "raw",
   "id": "interested-biotechnology",
   "metadata": {
    "tags": []
   },
   "source": [
    "# -----------------------------------------------------------------------------\n",
    "# Train function call\n",
    "def train():\n",
    "    return 0\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Generate list parameters to iterate over\n",
    "def parameter_grid_creation():\n",
    "    \n",
    "    return 0 #grid\n",
    "# -----------------------------------------------------------------------------\n",
    "# Grid search training & Evaluation\n",
    "def grid_search(self): # Method?\n",
    "    \"\"\"Method call using training method to perform a grid search across the \n",
    "    highlighted hyperparameters in the config file.\"\"\"\n",
    "    \n",
    "    # Set up dataframe for scoring hyperparameter combinations\n",
    "    df = pd.DataFrame()\n",
    "    df.write()\n",
    "    \n",
    "    # Create grid of hyperparamters to sample across\n",
    "    grid = parameter_grid_creation() # list of dicts\n",
    "    for parameters in grid:\n",
    "        model_ = train(**parameters)\n",
    "        parameter_performance = model_.valid_performance()\n",
    "        df.append(parameters, parameter_performance)\n",
    "    \n",
    "    # Find optimal hyperparameters\n",
    "    # Update config with optimised hyperparameters\n",
    "    config['grid_search']['done'] = 'True'\n",
    "    config['final_parameters']    = best_params\n",
    "    return dict_hyperparameter_selection\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Train final model and update config\n",
    "train()\n",
    "\n",
    "\"\"\"\n",
    "Unanswered Questions:\n",
    "   - Do I need early stopping?\n",
    "   - Implement system for training an ensemble trained via k-fold? \n",
    "   (with test data sub-set?)\n",
    "   - Should I decrease the size of the validation set anyways? Make this a grid searchable parameter?\n",
    "   - Should I implement random grid sampling for optimisation? (instead of sampling all of the combinations)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "raw",
   "id": "heated-combat",
   "metadata": {},
   "source": [
    "Data Manipulation:\n",
    "- path_to_model(file_name)\n",
    "- data_call(dataset_name)\n",
    "- determine_dataset(dataset,model_name) ... dataset in ['automatic','FRDEEP-F','MiraBest']\n",
    "\n",
    "Model Manipulation:\n",
    "- load_net(model_name,device)\n",
    "- training_validation(PATH,xlims=[None,None],save=False,full_path=False) ... PATH is a local title of a folder or file (within ./TrainedNetworks)\n",
    "- prediction(dataset, net, class_groups,(device='cuda',reps='360'))\n",
    "- evaluate(file_name, dataset='automatic')\n",
    "\n",
    "Evaluation Plots:\n",
    "- plot_conf_mat(conf_matrix,normalised=True,n_classes=2,format_input=None,title='Confusion Matrix')\n",
    "- plot_roc_curve(fpr,tpr,title='ROC Curve (AUC=\\{auc:.3f\\})')\n",
    "- out_print(out)\n",
    "\n",
    "Attention Maps:\n",
    "- attentions_func(batch_of_images, net, mean=True, device=torch.device('cpu'))\n",
    "- attention_analysis(source, source_only=True, attention_maps=None, GradCAM=None)\n",
    "- AttentionImagesByEpoch(sources, folder_name, net,epoch=1500, device=torch.device('cpu'))\n",
    "- attention_epoch_plot(source_images,folder_name, logged=False, width=3, device=torch.device('cpu'))\n",
    "\n",
    "GradCAM:\n",
    "- To be completed.\n",
    "\n",
    "Other:\n",
    "- mask_on_image(img, mask)\n",
    "- SortedDataSamples(data_name, transformed=True,  rotations=1, subset='NOHYBRID')\n",
    "- net_name_extraction(PATH)\n",
    "\n",
    "Incomplete:\n",
    "- Loading from Pickled dicts\n",
    "- GradCAM Call for a given image"
   ]
  },
  {
   "cell_type": "raw",
   "id": "honest-incidence",
   "metadata": {},
   "source": [
    "print(torch.__version__)\n",
    "print(torch._C._cuda_getCompiledVersion())"
   ]
  },
  {
   "cell_type": "raw",
   "id": "expired-permit",
   "metadata": {},
   "source": [
    "class Model:\n",
    "    def __init__(self, configfile):\n",
    "        # Read in the config file\n",
    "        self.config_name = configfile\n",
    "        self.config = ConfigParser.SafeConfigParser(allow_no_value=True)\n",
    "        self.config.read(self.config_name)\n",
    "        \n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "        self.net = utils.net.load(\n",
    "            device=self.device, \n",
    "            **self.config['model'])\n",
    "        self.data = utils.data.load(\n",
    "            device=self.device, \n",
    "            rotations=self.config.getint('model', 'number_rotations'), \n",
    "            **self.config['data'])\n",
    "        self.train = utils.train.load(self.config_name)\n",
    "        \n",
    "        self.model_trained = self.config.getboolean('grid_search', 'done')\n",
    "        \n",
    "\n",
    "    def model_trained(self):\n",
    "        if self.config.getboolean('grid_search', 'done'):\n",
    "            print('Training not required')\n",
    "        else:\n",
    "            print('Training required')\n",
    "\n",
    "test_net = Model('configs/template.cfg')\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "intimate-means",
   "metadata": {},
   "source": [
    "lr = model.config['grid_search']['learning_rate'].split(',')[1:]"
   ]
  },
  {
   "cell_type": "raw",
   "id": "faced-heading",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc-autonumbering": true,
  "toc-showcode": true,
  "toc-showmarkdowntxt": true
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
