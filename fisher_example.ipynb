{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "suspended-buddy",
   "metadata": {},
   "source": [
    "This notebook demonstrates using the Fisher Information to calculate generalisability and trainability metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "running-furniture",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import transforms\n",
    "import PIL\n",
    "\n",
    "from torchsummary import summary\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import configparser as ConfigParser\n",
    "\n",
    "\n",
    "import utils\n",
    "# Ipmport various network architectures\n",
    "from networks import AGRadGalNet, VanillaLeNet, testNet, DNSteerableLeNet, DNSteerableAGRadGalNet\n",
    "# Import various data classes\n",
    "from datasets import FRDEEPF\n",
    "from datasets import MiraBest_full, MBFRConfident, MBFRUncertain, MBHybrid\n",
    "from datasets import MingoLoTSS, MLFR, MLFRTest"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "quick-ordinary",
   "metadata": {},
   "source": [
    "Load in the dataset and the relevant configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "after-luther",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/raid/scratch/mattyb/venv/lib/python3.8/site-packages/torchvision/transforms/transforms.py:1314: UserWarning: Argument resample is deprecated and will be removed since v0.10.0. Please, use interpolation instead\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "\n",
    "PATH = \"configs/\"\n",
    "cfg_base = \"C4_attention_mirabest.cfg\"\n",
    "config = ConfigParser.ConfigParser(allow_no_value=True)\n",
    "config.read(PATH + cfg_base)\n",
    "device = \"cuda\"\n",
    "train_loader, valid_loader  = utils.data.load(\n",
    "    config, \n",
    "    train=True, \n",
    "    augmentation='config', \n",
    "    data_loader=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "valid-cutting",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading in DNSteerableAGRadGalNet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/raid/scratch/mattyb/venv/lib/python3.8/site-packages/e2cnn/nn/modules/r2_conv/basisexpansion_singleblock.py:61: UserWarning: indexing with dtype torch.uint8 is now deprecated, please use a dtype torch.bool instead. (Triggered internally at  /pytorch/aten/src/ATen/native/IndexingUtils.h:30.)\n",
      "  sampled_basis = sampled_basis[mask, ...]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "print(f\"Loading in {config['model']['base']}\")\n",
    "net = locals()[config['model']['base']](**config['model']).to(device)\n",
    "\n",
    "quiet = config.getboolean('DEFAULT', 'quiet')\n",
    "early_stopping = config.getboolean('training', 'early_stopping')\n",
    "\n",
    "# Read / Create Folder for Data to be Saved\n",
    "root = config['data']['directory']\n",
    "os.makedirs(root, exist_ok=True)\n",
    "\n",
    "if not quiet:\n",
    "    if 'DN' not in config['model']['base']:\n",
    "        summary(net, (1, 150, 150))\n",
    "    print(device)\n",
    "    if device == torch.device('cuda'):\n",
    "        print(torch.cuda.get_device_name(device=device))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "compound-bennett",
   "metadata": {},
   "source": [
    "Attempt to load in the best saved model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "minute-remedy",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_supliment = config['data']['augment']+'/'\n",
    "model = utils.utils.load_model(config, load_model='best', device=device, path_supliment=path_supliment)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eligible-refund",
   "metadata": {},
   "source": [
    "Save the Model Weights and then Train the Last Layer to ensure the grad information is retained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "incoming-balance",
   "metadata": {},
   "outputs": [],
   "source": [
    "Conv1a = model.conv1a.weights\n",
    "Conv1b = model.conv1b.weights\n",
    "Conv1c = model.conv1c.weights\n",
    "Conv2a = model.conv2a.weights\n",
    "Conv2b = model.conv2b.weights\n",
    "Conv2c = model.conv2c.weights\n",
    "Conv3a = model.conv3a.weights\n",
    "Conv3b = model.conv3b.weights\n",
    "Conv3c = model.conv3c.weights\n",
    "Conv4a = model.conv4a.weights\n",
    "Conv4b = model.conv4b.weights\n",
    "Psi1 = model.attention1.psi.weight\n",
    "Psi2 = model.attention2.psi.weight\n",
    "Psi3 = model.attention3.psi.weight\n",
    "Theta1 = model.attention1.theta.weight\n",
    "Theta2 = model.attention2.theta.weight\n",
    "Theta3 = model.attention3.theta.weight\n",
    "Phi1 = model.attention1.phi.weight\n",
    "Phi2 = model.attention2.phi.weight\n",
    "Phi3 = model.attention3.phi.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "united-authority",
   "metadata": {},
   "outputs": [],
   "source": [
    "net.conv1a.weights=Conv1a \n",
    "net.conv1b.weights=Conv1b \n",
    "net.conv1c.weights=Conv1c \n",
    "net.conv2a.weights=Conv2a \n",
    "net.conv2b.weights=Conv2b \n",
    "net.conv2c.weights=Conv2c \n",
    "net.conv3a.weights=Conv3a \n",
    "net.conv3b.weights=Conv3b \n",
    "net.conv3c.weights=Conv3c \n",
    "net.conv4a.weights=Conv4a \n",
    "net.conv4b.weights=Conv4b \n",
    "net.attention1.psi.weight=Psi1  \n",
    "net.attention2.psi.weight=Psi2  \n",
    "net.attention3.psi.weight=Psi3 \n",
    "net.attention1.theta.weight=Theta1 \n",
    "net.attention2.theta.weight=Theta2 \n",
    "net.attention3.theta.weight=Theta3 \n",
    "net.attention1.phi.weight=Phi1 \n",
    "net.attention2.phi.weight=Phi2 \n",
    "net.attention3.phi.weight=Phi3\n",
    "net.conv1a.weights.requires_grad=False\n",
    "net.conv1b.weights.requires_grad=False\n",
    "net.conv1c.weights.requires_grad=False\n",
    "net.conv2a.weights.requires_grad=False\n",
    "net.conv2b.weights.requires_grad=False\n",
    "net.conv2c.weights.requires_grad=False\n",
    "net.conv3a.weights.requires_grad=False\n",
    "net.conv3b.weights.requires_grad=False\n",
    "net.conv3c.weights.requires_grad=False\n",
    "net.conv4a.weights.requires_grad=False\n",
    "net.conv4b.weights.requires_grad=False\n",
    "net.attention1.psi.weight.requires_grad=False\n",
    "net.attention2.psi.weight.requires_grad=False\n",
    "net.attention3.psi.weight.requires_grad=False\n",
    "net.attention1.theta.weight.requires_grad=False\n",
    "net.attention2.theta.weight.requires_grad=False\n",
    "net.attention3.theta.weight.requires_grad=False\n",
    "net.attention1.phi.weight.requires_grad=False\n",
    "net.attention2.phi.weight.requires_grad=False\n",
    "net.attention3.phi.weight.requires_grad=False\n",
    "del(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "light-prediction",
   "metadata": {},
   "source": [
    "Test a Simplified Version of the Training Cycle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "confirmed-contrary",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(net, \n",
    "          device, \n",
    "          config,\n",
    "          Epoch,\n",
    "          train_loader,\n",
    "          valid_loader,\n",
    "          optimizer,\n",
    "          root_out_directory_addition='',\n",
    "          scheduler = None,\n",
    "          save_validation_updates=True,\n",
    "          class_splitting_index=1,\n",
    "          loss_function = nn.CrossEntropyLoss(),\n",
    "          output_model=True,\n",
    "          early_stopping=True,\n",
    "          output_best_validation=False,\n",
    "          stop_after_epochs_without_update=2000\n",
    "         ):\n",
    "    \"\"\"Very Simple version of the training loop used in the train.py file to try and find approximate\n",
    "    gradients of the Classifier Layer\n",
    "    \"\"\"\n",
    "    # -----------------------------------------------------------------------------\n",
    "    # Initialise Seeds\n",
    "    torch.manual_seed(42)\n",
    "    np.random.seed(42)\n",
    "    # -----------------------------------------------------------------------------\n",
    "    # Training Loop\n",
    "    validation_loss_min = np.Inf\n",
    "    for epoch_count in range(Epoch):\n",
    "    \n",
    "        # Model Training\n",
    "        train_loss = 0.\n",
    "        validation_loss = 0.\n",
    "        confussion_matrix = np.zeros((2,2))\n",
    "        net.train() #Set network to train mode.\n",
    "        if 'binary_labels' in locals():\n",
    "            del binary_labels\n",
    "        if 'outputs' in locals():\n",
    "            del outputs\n",
    "\n",
    "        # Loop across data augmentations\n",
    "        for batch_idx , (data, labels) in enumerate(train_loader): #Iterates through each batch.\n",
    "            data = data.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "                # Create binary labels to remove morphological subclassifications (for MiraBest) ### IS THIS STILL NECESSARY?\n",
    "            binary_labels = np.zeros(labels.size(), dtype=int)\n",
    "            binary_labels = np.where(labels.cpu().numpy()<class_splitting_index, binary_labels, binary_labels+1)\n",
    "            binary_labels = torch.from_numpy(binary_labels).to(device)\n",
    "                \n",
    "                # Loss & backpropagation\n",
    "            pred = net.forward(data)\n",
    "            optimizer.zero_grad()\n",
    "            loss = loss_function(pred,binary_labels)\n",
    "            loss.backward(retain_graph=True)\n",
    "            if scheduler == None:\n",
    "                optimizer.step()\n",
    "            train_loss += (loss.item()*data.size(0))\n",
    "            if scheduler != None:\n",
    "                scheduler.step(train_loss)\n",
    "        print(train_loss/(len(train_loader.dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "olive-malaysia",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(filter(lambda p: p.requires_grad, net.parameters()), lr=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "accomplished-texture",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/raid/scratch/mattyb/venv/lib/python3.8/site-packages/torch/nn/functional.py:3451: UserWarning: Default upsampling behavior when mode=bilinear is changed to align_corners=False since 0.4.0. Please specify align_corners=True if the old behavior is desired. See the documentation of nn.Upsample for details.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5352245236144346\n",
      "0.5429738830117619\n",
      "0.5884865347076865\n",
      "0.5343444137012258\n",
      "0.5093576487372903\n",
      "0.5588863211519578\n",
      "0.5623948048142826\n",
      "0.5237291630576638\n",
      "0.5422553209697499\n",
      "0.5166515392415664\n",
      "0.5582884795525495\n",
      "0.5428332090377808\n",
      "0.5369892085299772\n",
      "0.5197013301007888\n",
      "0.5242063368068022\n",
      "0.5304787264150732\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-d294af2520de>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-8-1946e63f60f3>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(net, device, config, train_loader, valid_loader, optimizer, root_out_directory_addition, scheduler, save_validation_updates, class_splitting_index, loss_function, output_model, early_stopping, output_best_validation, stop_after_epochs_without_update)\u001b[0m\n\u001b[1;32m     55\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mscheduler\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m                 \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 57\u001b[0;31m             \u001b[0mtrain_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     58\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mscheduler\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m                 \u001b[0mscheduler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train(net, device, config, 10, train_loader, valid_loader, optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "announced-hundred",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-9.1327e-04,  1.4837e-32, -4.2458e-04, -5.1269e-04, -5.6729e-03,\n",
      "          0.0000e+00],\n",
      "        [ 9.1319e-04,  0.0000e+00,  4.2454e-04,  5.1264e-04,  5.6725e-03,\n",
      "          0.0000e+00]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "print(net.classifier.weight.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "enabling-jewel",
   "metadata": {},
   "source": [
    "Now Fisher and Jacobian Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "promising-mozambique",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
