{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "polished-kidney",
   "metadata": {},
   "source": [
    "This notebook demonstrates using the Fisher Information to calculate generalisability and trainability metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "honest-replica",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fatal: destination path 'MiraBest' already exists and is not an empty directory.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import transforms\n",
    "import PIL\n",
    "\n",
    "from torchsummary import summary\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import configparser as ConfigParser\n",
    "\n",
    "\n",
    "import utils\n",
    "# Ipmport various network architectures\n",
    "from networks import AGRadGalNet, VanillaLeNet, testNet, DNSteerableLeNet, DNSteerableAGRadGalNet\n",
    "# Import various data classes\n",
    "from datasets import FRDEEPF\n",
    "from datasets import MiraBest_full, MBFRConfident, MBFRUncertain, MBHybrid\n",
    "from datasets import MingoLoTSS, MLFR, MLFRTest\n",
    "\n",
    "!git clone https://github.com/fmporter/MiraBest\n",
    "from MiraBest import MiraBest"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "extensive-tribute",
   "metadata": {},
   "source": [
    "Load in the dataset and the relevant configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "sound-cincinnati",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/raid/scratch/mattyb/venv/lib/python3.8/site-packages/torchvision/transforms/transforms.py:1314: UserWarning: Argument resample is deprecated and will be removed since v0.10.0. Please, use interpolation instead\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "\n",
    "PATH = \"configs/\"\n",
    "cfg_base = \"C4_attention_mirabest.cfg\"\n",
    "config = ConfigParser.ConfigParser(allow_no_value=True)\n",
    "config.read(PATH + cfg_base)\n",
    "device = \"cuda\"\n",
    "\n",
    "train_loader, valid_loader  = utils.data.load(\n",
    "    config, \n",
    "    train=True, \n",
    "    augmentation='config', \n",
    "    data_loader=True\n",
    ")\n",
    "\n",
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(),\n",
    "     transforms.Normalize([0.5],[0.5])])\n",
    "trainset = MiraBest.MiraBest(root='./batches', train=True, download=True, transform=transform)  \n",
    "batch_size_train = 1\n",
    "single_train_loader = torch.utils.data.DataLoader(trainset, batch_size=batch_size_train, shuffle=False, num_workers=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "loved-setup",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading in DNSteerableAGRadGalNet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/raid/scratch/mattyb/venv/lib/python3.8/site-packages/e2cnn/nn/modules/r2_conv/basisexpansion_singleblock.py:61: UserWarning: indexing with dtype torch.uint8 is now deprecated, please use a dtype torch.bool instead. (Triggered internally at  /pytorch/aten/src/ATen/native/IndexingUtils.h:30.)\n",
      "  sampled_basis = sampled_basis[mask, ...]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "print(f\"Loading in {config['model']['base']}\")\n",
    "net = locals()[config['model']['base']](**config['model']).to(device)\n",
    "\n",
    "quiet = config.getboolean('DEFAULT', 'quiet')\n",
    "early_stopping = config.getboolean('training', 'early_stopping')\n",
    "\n",
    "# Read / Create Folder for Data to be Saved\n",
    "root = config['data']['directory']\n",
    "os.makedirs(root, exist_ok=True)\n",
    "\n",
    "if not quiet:\n",
    "    if 'DN' not in config['model']['base']:\n",
    "        summary(net, (1, 150, 150))\n",
    "    print(device)\n",
    "    if device == torch.device('cuda'):\n",
    "        summary(net, (1,150,150))\n",
    "        print(torch.cuda.get_device_name(device=device))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "prerequisite-interval",
   "metadata": {},
   "source": [
    "Attempt to load in the best saved model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "based-export",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_supliment = config['data']['augment']+'/'\n",
    "model = utils.utils.load_model(config, load_model='best', device=device, path_supliment=path_supliment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "lesser-prague",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mask.mask\n",
      "conv1a.weights\n",
      "bnorm1a.batch_norm_[4].weight\n",
      "bnorm1a.batch_norm_[4].bias\n",
      "conv1b.weights\n",
      "bnorm1b.batch_norm_[4].weight\n",
      "bnorm1b.batch_norm_[4].bias\n",
      "conv1c.weights\n",
      "bnorm1c.batch_norm_[4].weight\n",
      "bnorm1c.batch_norm_[4].bias\n",
      "conv2a.weights\n",
      "bnorm2a.batch_norm_[4].weight\n",
      "bnorm2a.batch_norm_[4].bias\n",
      "conv2b.weights\n",
      "bnorm2b.batch_norm_[4].weight\n",
      "bnorm2b.batch_norm_[4].bias\n",
      "conv2c.weights\n",
      "bnorm2c.batch_norm_[4].weight\n",
      "bnorm2c.batch_norm_[4].bias\n",
      "conv3a.weights\n",
      "bnorm3a.batch_norm_[4].weight\n",
      "bnorm3a.batch_norm_[4].bias\n",
      "conv3b.weights\n",
      "bnorm3b.batch_norm_[4].weight\n",
      "bnorm3b.batch_norm_[4].bias\n",
      "conv3c.weights\n",
      "bnorm3c.batch_norm_[4].weight\n",
      "bnorm3c.batch_norm_[4].bias\n",
      "conv4a.weights\n",
      "bnorm4a.batch_norm_[4].weight\n",
      "bnorm4a.batch_norm_[4].bias\n",
      "conv4b.weights\n",
      "bnorm4b.batch_norm_[4].weight\n",
      "bnorm4b.batch_norm_[4].bias\n",
      "attention1.theta.weight\n",
      "attention1.phi.weight\n",
      "attention1.psi.weight\n",
      "attention1.psi.bias\n",
      "attention2.theta.weight\n",
      "attention2.phi.weight\n",
      "attention2.psi.weight\n",
      "attention2.psi.bias\n",
      "attention3.theta.weight\n",
      "attention3.phi.weight\n",
      "attention3.psi.weight\n",
      "attention3.psi.bias\n",
      "fc1.weight\n",
      "fc1.bias\n",
      "fc2.weight\n",
      "fc2.bias\n",
      "fc3.weight\n",
      "fc3.bias\n",
      "classifiers.0.weight\n",
      "classifiers.0.bias\n",
      "classifiers.1.weight\n",
      "classifiers.1.bias\n",
      "classifiers.2.weight\n",
      "classifiers.2.bias\n",
      "classifier.weight\n",
      "classifier.bias\n"
     ]
    }
   ],
   "source": [
    "for name, param in model.named_parameters():\n",
    "    print(name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "musical-india",
   "metadata": {},
   "source": [
    "Save the Model Weights and then Train the Last Layer to ensure the grad information is retained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "indoor-wednesday",
   "metadata": {},
   "outputs": [],
   "source": [
    "Conv1a = model.conv1a.weights\n",
    "Conv1b = model.conv1b.weights\n",
    "Conv1c = model.conv1c.weights\n",
    "Conv2a = model.conv2a.weights\n",
    "Conv2b = model.conv2b.weights\n",
    "Conv2c = model.conv2c.weights\n",
    "Conv3a = model.conv3a.weights\n",
    "Conv3b = model.conv3b.weights\n",
    "Conv3c = model.conv3c.weights\n",
    "Conv4a = model.conv4a.weights\n",
    "Conv4b = model.conv4b.weights\n",
    "Psi1 = model.attention1.psi.weight\n",
    "Psi2 = model.attention2.psi.weight\n",
    "Psi3 = model.attention3.psi.weight\n",
    "Theta1 = model.attention1.theta.weight\n",
    "Theta2 = model.attention2.theta.weight\n",
    "Theta3 = model.attention3.theta.weight\n",
    "Phi1 = model.attention1.phi.weight\n",
    "Phi2 = model.attention2.phi.weight\n",
    "Phi3 = model.attention3.phi.weight\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "upper-bunny",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "net.conv1a.weights=Conv1a \n",
    "net.conv1b.weights=Conv1b \n",
    "net.conv1c.weights=Conv1c \n",
    "net.conv2a.weights=Conv2a \n",
    "net.conv2b.weights=Conv2b \n",
    "net.conv2c.weights=Conv2c \n",
    "net.conv3a.weights=Conv3a \n",
    "net.conv3b.weights=Conv3b \n",
    "net.conv3c.weights=Conv3c \n",
    "net.conv4a.weights=Conv4a \n",
    "net.conv4b.weights=Conv4b \n",
    "net.attention1.psi.weight=Psi1  \n",
    "net.attention2.psi.weight=Psi2  \n",
    "net.attention3.psi.weight=Psi3 \n",
    "net.attention1.theta.weight=Theta1 \n",
    "net.attention2.theta.weight=Theta2 \n",
    "net.attention3.theta.weight=Theta3 \n",
    "net.attention1.phi.weight=Phi1 \n",
    "net.attention2.phi.weight=Phi2 \n",
    "net.attention3.phi.weight=Phi3\n",
    "\n",
    "del(model)\n",
    "\n",
    "for name, param in net.named_parameters():\n",
    "    param.requires_grad=False\n",
    "    \n",
    "net.classifier.weight.requires_grad=True\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "transsexual-devon",
   "metadata": {},
   "source": [
    "Test a Simplified Version of the Training Cycle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dutch-transformation",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(net, \n",
    "          device, \n",
    "          config,\n",
    "          Epoch,\n",
    "          train_loader,\n",
    "          optimizer,\n",
    "          root_out_directory_addition='',\n",
    "          scheduler = None,\n",
    "          save_validation_updates=True,\n",
    "          class_splitting_index=1,\n",
    "          loss_function = nn.CrossEntropyLoss(),\n",
    "          output_model=True,\n",
    "          early_stopping=True,\n",
    "          output_best_validation=False,\n",
    "          stop_after_epochs_without_update=2000\n",
    "         ):\n",
    "    \"\"\"Very Simple version of the training loop used in the train.py file to try and find approximate\n",
    "    gradients of the Classifier Layer\n",
    "    \"\"\"\n",
    "    # -----------------------------------------------------------------------------\n",
    "    # Initialise Seeds\n",
    "    torch.manual_seed(42)\n",
    "    np.random.seed(42)\n",
    "    # -----------------------------------------------------------------------------\n",
    "    # Training Loop\n",
    "    validation_loss_min = np.Inf\n",
    "    for epoch_count in range(Epoch):\n",
    "    \n",
    "        # Model Training\n",
    "        train_loss = 0.\n",
    "        validation_loss = 0.\n",
    "        confussion_matrix = np.zeros((2,2))\n",
    "        net.train() #Set network to train mode.\n",
    "        if 'binary_labels' in locals():\n",
    "            del binary_labels\n",
    "        if 'outputs' in locals():\n",
    "            del outputs\n",
    "\n",
    "        # Loop across data augmentations\n",
    "        for batch_idx , (data, labels) in enumerate(train_loader): #Iterates through each batch.\n",
    "            data = data.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            \n",
    "            # Create binary labels to remove morphological subclassifications (for MiraBest) ### IS THIS STILL NECESSARY?\n",
    "            binary_labels = np.zeros(labels.size(), dtype=int)\n",
    "            binary_labels = np.where(labels.cpu().numpy()<class_splitting_index, binary_labels, binary_labels+1)\n",
    "            binary_labels = torch.from_numpy(binary_labels).to(device)\n",
    "            \n",
    "                # Loss & backpropagation\n",
    "            pred = net.forward(data)\n",
    "            optimizer.zero_grad()\n",
    "            loss = loss_function(pred,binary_labels)\n",
    "            loss.backward(retain_graph=True)\n",
    "            if scheduler == None:\n",
    "                optimizer.step()\n",
    "            train_loss += (loss.item()*data.size(0))\n",
    "            if scheduler != None:\n",
    "                scheduler.step(train_loss)\n",
    "        print(train_loss/(len(train_loader.dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "confused-quantity",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(filter(lambda p: p.requires_grad, net.parameters()), lr=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "polish-buying",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/raid/scratch/mattyb/venv/lib/python3.8/site-packages/torch/nn/functional.py:3451: UserWarning: Default upsampling behavior when mode=bilinear is changed to align_corners=False since 0.4.0. Please specify align_corners=True if the old behavior is desired. See the documentation of nn.Upsample for details.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5352304753135232\n",
      "0.5429462685304529\n",
      "0.5889544557122623\n",
      "0.535029460402096\n",
      "0.5095707230708179\n",
      "0.5586375874631545\n",
      "0.5629613294320948\n",
      "0.5248154051163617\n",
      "0.5431032882017248\n",
      "0.5163005870931289\n"
     ]
    }
   ],
   "source": [
    "train(net, device, config, 10, train_loader, optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "italian-navigator",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-1.8417e-05,  5.5916e-14, -5.1928e-07,  0.0000e+00, -2.1291e-04,\n",
      "          0.0000e+00],\n",
      "        [ 1.8433e-05,  0.0000e+00,  5.1998e-07,  0.0000e+00,  2.1310e-04,\n",
      "          0.0000e+00]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "print(net.classifier.weight.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "combined-separate",
   "metadata": {},
   "source": [
    "Now Fisher and Jacobian Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "meaningful-retrieval",
   "metadata": {},
   "outputs": [],
   "source": [
    "def jacobian_torch(f, w, create_graph=False):   \n",
    "\n",
    "    \"\"\"\n",
    "    function to find the jacobian of f with respect to x parameters of model\n",
    "\n",
    "    output has shape (len(f), len(x))\n",
    "    \"\"\"\n",
    "\n",
    "    jac = []                                                                                          \n",
    "    grad_f = torch.zeros_like(f)\n",
    "\n",
    "    for i in range(len(f)):                                                                      \n",
    "        grad_f[i] = 1.\n",
    "        grad_f_x = torch.autograd.grad(f, w, grad_f, retain_graph=True, create_graph=create_graph, allow_unused=True)\n",
    "        J = torch.cat(grad_f_x).view(-1)\n",
    "        jac.append(J)\n",
    "        grad_f[i] = 0.\n",
    "                                                  \n",
    "    return torch.stack(jac).reshape(f.shape + J.shape)       \n",
    "\n",
    "def jacobian(y, x, create_graph=False):                                                               \n",
    "    jac = []                                                                                          \n",
    "    flat_y = y.reshape(-1)                                                                            \n",
    "    grad_y = torch.zeros_like(flat_y)                                                                 \n",
    "    for i in range(len(flat_y)):                                                                      \n",
    "        grad_y[i] = 1.                                                                                \n",
    "        grad_x, = torch.autograd.grad(flat_y, x, grad_y, retain_graph=True, create_graph=create_graph, allow_unused=True)\n",
    "        jac.append(grad_x.reshape(x.shape))                                                           \n",
    "        grad_y[i] = 0.                                                                                \n",
    "    return torch.stack(jac).reshape(y.shape + x.shape)\n",
    "\n",
    "def matrix_diag(diagonal):\n",
    "    N = diagonal.shape[-1]\n",
    "    shape = diagonal.shape[:-1] + (N, N)\n",
    "    device, dtype = diagonal.device, diagonal.dtype\n",
    "    result = torch.zeros(shape, dtype=dtype, device=device)\n",
    "    indices = torch.arange(result.numel(), device=device).reshape(shape)\n",
    "    indices = indices.diagonal(dim1=-2, dim2=-1)\n",
    "    result.view(-1)[indices] = diagonal\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "documentary-proportion",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [01:56<00:00,  1.16s/it]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "Number_of_FisherIts = 100\n",
    "Rank = []\n",
    "EV = []\n",
    "FR = []\n",
    "n_weight = 12\n",
    "realisations_torch = torch.zeros((Number_of_FisherIts,n_weight,n_weight)).to(device) #All fishers\n",
    "for i in tqdm(range(Number_of_FisherIts)):\n",
    "    \n",
    "    w = net.classifier.weight\n",
    "    flat_w = w.view(-1).cpu()\n",
    "\n",
    "    fisher = torch.zeros((n_weight,n_weight)).to('cuda')\n",
    "    for x_n, y_n in train_loader:\n",
    "        x_n, y_n = x_n.to('cuda'), y_n.to('cuda')\n",
    "\n",
    "        f_n = net(x_n)\n",
    "        for row in f_n:\n",
    "            m = nn.Softmax(dim=0)\n",
    "            pi_n = m(row)\n",
    "            diag_pi_n = torch.diag(pi_n.squeeze(0)).to('cuda')\n",
    "            pi_n_pi_n_T= torch.from_numpy(np.outer(pi_n.cpu().detach().numpy(),pi_n.cpu().detach().numpy())).to('cuda')\n",
    "            J_f = jacobian_torch((torch.squeeze(row,0)),w)\n",
    "            J_f_T = J_f.permute(1,0)\n",
    "            K2 = diag_pi_n-pi_n_pi_n_T\n",
    "            K3 = K2.cuda()\n",
    "            fisher += torch.matmul((torch.matmul(J_f_T,K3)),J_f)\n",
    "  \n",
    "    with torch.no_grad():\n",
    "        rank = torch.matrix_rank(fisher).item()\n",
    "        Rank.append(rank)\n",
    "        realisations_torch[i] = fisher.cpu()\n",
    "        Fw = np.matmul(fisher.cpu().numpy(),flat_w)\n",
    "        wFw = np.dot(flat_w,Fw)\n",
    "        FR.append(wFw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "sporting-substance",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.53\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.subplot(111)\n",
    "nbins = 10\n",
    "plt.hist(Rank, bins=nbins, rwidth=0.8, color='g')\n",
    "plt.ylabel(\"Total Counts\")\n",
    "plt.xlabel(\"Matrix Rank\")\n",
    "plt.title(\"Fisher Matrix Rank Distribution\")\n",
    "print(np.mean(Rank))\n",
    "plt.savefig('VanillaRank.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "specialized-animal",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
